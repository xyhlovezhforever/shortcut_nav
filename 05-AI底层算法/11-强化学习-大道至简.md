JustRL: Scaling a 1.5B LLM with a Simple RL Recipe

论文地址：https://arxiv.org/abs/2512.16649

代码地址：https: //github.com/thunlp/JustRL

创新点：
• 把“简单”做到极致——用单阶段、固定超参、无技巧的纯 RL 就能把 1.5 B 模型推到 SOTA，反而比堆复杂 trick 更稳、更省、更强。

• 把业界常用的“标准技巧”——显式长度惩罚、鲁棒验证器、动态课程——逐个加回去，反而出现探索坍缩或性能下降；说明复杂 trick 并非“必要”，有时甚至“有害”。

方法：
本文的核心研究方法就是把“极简”坚持到底：直接在 1.5 B 参数规模的基座模型上做单阶段 PPO，全程固定学习率、KL 系数、裁剪阈值等超参数，不引入任何课程学习、动态调度、长度惩罚或鲁棒验证器，仅依靠足够大的批量（batch）和更新步数（4 k+ steps）让策略在奖励信号下平滑、单调地提升，最终用这一套“零技巧”配方同时训练出两个不同初始化但结构相同的 1.5 B 模型，并在九个数学推理基准上取得 SOTA 水平，借此验证“简单放大”本身就能解决以往靠复杂工程手段才压得住的训练不稳定问题。

JustRL 1.5B 模型在数学推理任务上的训练曲线与性能对比

本图通过两条随训练步数单调上升的准确率曲线，直观呈现了 JustRL 在 DeepSeek-1.5B 与 Nemotron-1.5B 上的“无技巧”强化学习过程：仅采用单阶段固定超参 PPO，未引入课程、动态调度或长度惩罚，即可在 4 000 步内将平均准确率分别从约 28% 与 56% 的基线水平提升至 58.59% 与 73.33%，显著超越同规模对照模型（BroRL、OpenMath、QuestA 及 R1-Distill-Qwen）并刷新 1.5B 量级在九项数学基准上的最佳纪录；曲线平滑无震荡，表明随着样本量与更新步数的扩大，策略在奖励信号驱动下持续稳定地收敛，验证了“简单放大”本身足以克服以往需复杂工程干预才能缓解的训练不稳定性，从而支持论文核心结论——强化学习对大型语言模型的 scaling 未必依赖日益增长的算法复杂度。

JustRL 训练过程中策略熵、平均奖励与响应长度的动态演化

本图刻画了 JustRL 在 4 000 步单阶段强化学习训练中的内在动力学：策略熵（a）自初始 1.8 nats 附近呈单调递减并渐趋平稳，表明模型在探索-利用权衡中持续收紧策略分布；与此同时，平均奖励（b）从接近零的初始值稳步抬升至约 0.4，验证了奖励信号的有效传递与策略优化方向的正确性；而平均响应长度（c）则由初始约 3 800 tokens 逐步缩减至 4 000 步时的 5 000 tokens 以下，呈现出“先略增后缓降”的受控收缩，反映出模型在提升答案正确性的过程中自发学会了简洁表达，无需显式长度惩罚。三者协同演化且全程无突变或震荡，进一步证明在固定超参与大 batch scaling 的设定下，简单 PPO 即可驱动 1.5 B 模型实现稳定、高效且可解释的推理能力跃升。

引入过长惩罚与鲁棒验证器对 JustRL-DeepSeek-1.5B 训练动态的反作用效应

本图系统展示了在原有“无技巧”PPO 框架上分别叠加显式过长惩罚（Overlong Penalty）与进一步引入鲁棒验证器（Robust Verifier）后，模型在 AIME2024 准确率及训练动力学上的退化现象：AIME2024 准确率（a）由裸 JustRL 的约 0.55 跌至 0.45 附近，表明外部约束直接削弱最终推理性能；策略熵（b）在训练初期即被过度压制，并在 2000 步后持续低于基线，揭示探索空间被人为压缩；平均奖励（c）上升速度显著放缓且终值降低，说明奖励信号的有效传递受阻；平均响应长度（d）虽被迅速压缩至 3000 tokens 以下，却伴随正确率同步下滑，证明长度惩罚未能实现“简洁且正确”的期望，反而诱发模型以牺牲精度换取短输出。

实验

该表格汇总了 JustRL-DeepSeek-1.5B 与近期同规模强化学习推理模型在九项数学基准上的性能对比，结果显示 JustRL 在 AIME2024、AIME2025、AMC23、MATH、Minerva、Olympiad、HMMT、BRUMO 与 CMIMC 上分别取得 54.87%、25.63%、92.14%、49.08%、61.54%、52.71%、67.99%、38.75% 与 91.65% 的准确率，平均 60.49%，显著优于 Backbone 基线（37.65%）并全面超越 DeepScaleR-1.5B（40.21%）、ProRL-V2（47.29%）及 BroRL（51.47%），在保持 1.5 B 参数规模与单阶段固定超参训练设定下刷新了小规模模型的数学推理上限，验证了极简强化学习框架的 scaling 有效性。

