# 第二章：深度学习革命 - AI的觉醒（2012-2017）

## 章节概述

2012-2017年是AI历史上最激动人心的5年。AlexNet的横空出世标志着深度学习革命的开始，计算机视觉、自然语言处理、生成模型等领域相继取得突破性进展。这一时期，AI从学术界的研究课题变成了改变世界的技术力量。

**核心里程碑**：
- 2012：AlexNet - ImageNet突破
- 2013：Word2Vec - 词向量革命
- 2014：GAN - 生成模型崛起，VGGNet、GoogLeNet
- 2015：ResNet - 超深网络，Attention机制
- 2016：AlphaGo - 击败李世石
- 2017：Transformer - 注意力即一切

---

## 2.1 革命前夜：关键技术的积累（2006-2012）

深度学习的成功不是偶然的，而是建立在多年技术积累之上。

### Hinton的坚守：深度信念网络（2006）

#### 背景与动机

**2006年的学术环境**：
- 神经网络被主流学术界边缘化
- SVM等统计方法占据主导地位
- 深层网络被认为"无法训练"

**Geoffrey Hinton的坚持**：
- 自1970年代起研究神经网络
- 在AI寒冬中从未放弃
- 2006年，他58岁，发表了突破性工作

#### 深度信念网络（DBN）

**核心问题**：
如何训练深层神经网络？

**传统方法的失败**：
```
随机初始化 + 反向传播：
- 梯度消失导致前几层学不到东西
- 陷入局部最优
- 训练缓慢且不稳定
```

**Hinton的解决方案：逐层预训练**

**算法流程**：
```
第1步：训练第一层RBM（受限玻尔兹曼机）
输入层 ←→ 隐藏层1
使用对比散度（Contrastive Divergence）训练

第2步：固定第一层，训练第二层RBM
隐藏层1 ←→ 隐藏层2
将隐藏层1的激活作为输入

第3步：重复直到所有层
隐藏层2 ←→ 隐藏层3
...

第4步：微调（Fine-tuning）
使用反向传播微调整个网络
```

**受限玻尔兹曼机（RBM）**：
```
结构：
可见层v ←→ 隐藏层h
（层内无连接，层间全连接）

能量函数：
E(v,h) = -Σᵢaᵢvᵢ - Σⱼbⱼhⱼ - ΣᵢΣⱼvᵢwᵢⱼhⱼ

概率分布：
P(v,h) = exp(-E(v,h)) / Z

训练目标：
最大化可见层数据的似然

训练算法（对比散度CD-k）：
1. 正相：从数据v⁽⁰⁾开始，计算h⁽⁰⁾
2. 负相：从h⁽⁰⁾重构v⁽¹⁾，再计算h⁽¹⁾
3. 梯度：Δw ∝ E[vh]_data - E[vh]_model
```

**为什么逐层预训练有效？**

**理论解释**：
```
1. 无监督预训练学习数据的表示
   - 每层RBM学习该层的特征
   - 从低级特征到高级特征

2. 为监督学习提供良好初始化
   - 避免随机初始化的糟糕局部最优
   - 网络已经学到了有意义的表示

3. 正则化效果
   - 限制了参数搜索空间
   - 减少过拟合
```

**实验结果（2006）**：
```
MNIST手写数字识别：
- DBN：错误率1.25%
- SVM：1.4%
- 传统神经网络：1.6%

关键：证明了深层网络可以训练！
```

**意义**：
- 重新点燃了深度学习的希望
- 证明深层网络比浅层网络更强大
- 为2012年的突破奠定基础

**局限**：
```
1. 计算成本高：
   - 预训练每层RBM很慢
   - 需要大量迭代

2. 复杂性：
   - 训练过程复杂
   - 超参数多

3. 最终被更简单方法取代：
   - ReLU + Dropout + 数据增强
   - 不需要预训练
```

---

### ReLU激活函数的回归（2010）

#### 激活函数的演变

**早期：Sigmoid**
```python
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

特点：
- 输出范围：(0, 1)
- 可微
- 平滑

问题：
1. 梯度消失：
   - σ'(x) = σ(x)(1-σ(x)) ≤ 0.25
   - 深层网络梯度指数衰减

2. 非零中心：
   - 输出恒为正
   - 导致梯度方向不佳

3. 计算慢：
   - 指数运算
```

**改进：Tanh**
```python
def tanh(x):
    return (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))

特点：
- 输出范围：(-1, 1)
- 零中心

问题：
- 仍然有梯度消失
- tanh'(x) ≤ 1
```

#### ReLU的诞生

**Rectified Linear Unit（2010，Nair & Hinton）**：
```python
def relu(x):
    return np.maximum(0, x)

# 或者
def relu(x):
    return x if x > 0 else 0
```

**数学表达**：
```
ReLU(x) = max(0, x) = {
    x,  if x > 0
    0,  if x ≤ 0
}

导数：
ReLU'(x) = {
    1,  if x > 0
    0,  if x ≤ 0
}
```

**为什么ReLU这么有效？**

**1. 解决梯度消失**
```
对比：
Sigmoid：σ'(x) ≤ 0.25
Tanh：tanh'(x) ≤ 1
ReLU：ReLU'(x) = 1 (x>0时)

10层网络反向传播：
Sigmoid：梯度 ≤ 0.25¹⁰ ≈ 10⁻⁶（几乎消失）
ReLU：梯度 = 1（不衰减！）
```

**2. 计算高效**
```
Sigmoid：需要exp运算
ReLU：只需比较和选择

速度对比：
ReLU比Sigmoid快约6倍
```

**3. 稀疏激活**
```
现象：
- 约50%的神经元输出为0（x<0时）
- 网络变得稀疏

好处：
- 计算更快（跳过零激活）
- 类似dropout的正则化效果
- 类似生物神经元（不总是激活）
```

**4. 无饱和区（正半轴）**
```
Sigmoid/Tanh：
- x很大或很小时，导数≈0（饱和）
- 梯度消失

ReLU：
- x>0时，导数恒为1
- 梯度始终流动
```

**历史的讽刺**

**ReLU并非新发明**：
```
1960年代：
- 已在生物神经元模型中使用
- Kunihiko Fukushima的Neocognitron（1980）

为什么当时被忽视？
- 被认为"太简单"
- 不够"生物学真实"
- Sigmoid被认为更平滑、更优雅
```

**2010-2012年的关键证明**：
```
2010：Nair & Hinton在RBM中使用
2011：Xavier Glorot等人系统研究
2012：AlexNet大规模使用，证明有效性

结论：
有时候，简单就是美
```

#### ReLU的变体

**1. Leaky ReLU（2013）**
```python
def leaky_relu(x, alpha=0.01):
    return np.maximum(alpha * x, x)

# 等价于
def leaky_relu(x, alpha=0.01):
    return x if x > 0 else alpha * x
```

**动机**：
- 解决"dying ReLU"问题
- 当x<0时，ReLU梯度为0，神经元可能"死亡"
- Leaky ReLU给负值一个小梯度

**2. PReLU（参数化ReLU，2015）**
```python
def prelu(x, alpha):
    return np.maximum(alpha * x, x)

# alpha是可学习的参数，不同通道可以有不同alpha
```

**3. ELU（指数线性单元，2015）**
```python
def elu(x, alpha=1.0):
    return x if x > 0 else alpha * (np.exp(x) - 1)
```

**特点**：
- 负值时平滑
- 均值接近0（有助于训练）

**4. SELU（自归一化ELU，2017）**
- 特殊参数的ELU
- 具有自归一化性质

**实践中的选择**：
```
默认：ReLU
- 简单、快速、有效
- 适合大多数情况

需要负值激活：Leaky ReLU / PReLU
- 避免dying ReLU

需要平滑：ELU / SELU
- 在某些任务上更好
```

---

### Dropout：防止过拟合的魔法（2012）

#### Hinton的灵感

**传说中的灵感来源**：
> "银行要求员工频繁轮岗，以防止员工之间合谋欺诈。如果员工不知道下次会和谁一起工作，就不敢串通。"

**类比到神经网络**：
- 神经元不知道哪些"同事"会出现
- 被迫学习更鲁棒的特征
- 不能依赖特定的神经元组合

#### Dropout算法

**训练时**：
```python
def dropout_forward(x, dropout_prob, training=True):
    if not training:
        return x

    # 生成mask：以(1-dropout_prob)的概率保留
    mask = np.random.binomial(1, 1-dropout_prob, size=x.shape)

    # 应用mask
    out = x * mask

    # 缩放（保持期望不变）
    out = out / (1 - dropout_prob)

    return out, mask

def dropout_backward(dout, mask, dropout_prob):
    # 反向传播
    dx = dout * mask / (1 - dropout_prob)
    return dx
```

**测试时**：
```python
def dropout_forward(x, dropout_prob, training=False):
    # 测试时不dropout，直接使用所有神经元
    return x
```

**关键点：缩放（Scaling）**

**方案1：训练时缩放（Inverted Dropout）**
```python
# 训练时
out = out / (1 - dropout_prob)

# 测试时
out = x  # 不需要缩放
```

**方案2：测试时缩放**
```python
# 训练时
out = x * mask

# 测试时
out = x * (1 - dropout_prob)
```

**实践中通常用方案1**：
- 测试时无额外计算
- 推理更快

#### 为什么Dropout有效？

**解释1：集成学习（Ensemble）**
```
Dropout相当于训练指数级数量的子网络：

假设网络有n个神经元：
- 可能的子网络数量：2ⁿ
- 每次训练相当于训练一个子网络
- 测试时近似所有子网络的平均

例子：
n=1000个神经元
dropout_prob=0.5
子网络数量：2¹⁰⁰⁰ ≈ 10³⁰⁰（远超宇宙原子数）

相当于用计算一个网络的代价，
训练了指数级数量的网络！
```

**解释2：防止共适应（Co-adaptation）**
```
问题：
某些神经元过度依赖其他特定神经元
→ 形成"联盟"
→ 对训练数据过拟合

Dropout的作用：
- 随机删除神经元
- 强迫每个神经元学习独立有用的特征
- 不能依赖特定的"同伴"

类比：
团队成员随机缺席，
每个人必须具备独立工作能力
```

**解释3：等价于L2正则化（在线性模型中）**
```
理论研究表明：
在线性回归中，dropout等价于L2正则化

但在深度网络中，效果更强
```

**解释4：增加噪声的鲁棒性**
```
Dropout相当于在训练时添加噪声：
- 随机删除神经元 = 添加乘性噪声
- 迫使网络学习对噪声鲁棒的表示
```

#### Dropout的实践技巧

**1. Dropout率的选择**
```
经验法则：
- 全连接层：0.5（删除50%）
- 卷积层：0.1-0.2（删除较少）
- 输入层：0.1-0.2或不用

原因：
- 全连接层参数多，容易过拟合
- 卷积层有权值共享，正则化效应弱一些
```

**2. 何时使用Dropout**
```
使用：
- 网络容易过拟合（训练误差<<测试误差）
- 全连接层
- 数据量有限

不使用：
- 数据量充足
- 已有其他正则化（如BatchNorm）
- 小网络（欠拟合风险）
```

**3. Dropout vs Batch Normalization**
```
2015年后的发现：
BatchNorm也有正则化效果

常见组合：
Conv → BatchNorm → ReLU （不用Dropout）
...
FC → Dropout → ReLU
FC → Dropout → ReLU
```

**4. 训练技巧**
```
- 可能需要更多epochs
  （因为每次只训练部分网络）

- 可能需要更大的学习率
  （因为梯度被稀疏化）

- 可以增大网络容量
  （dropout提供正则化）
```

#### Dropout的变体

**1. DropConnect（2013）**
```
Dropout：随机删除神经元
DropConnect：随机删除连接（权重）

效果类似，但计算更复杂
实践中较少使用
```

**2. Spatial Dropout（2015）**
```
用于卷积层：
- 标准Dropout：随机删除单个激活
- Spatial Dropout：随机删除整个特征图

适合卷积网络
```

**3. DropBlock（2018）**
```
进一步改进的Spatial Dropout
删除连续的区域，而非单个像素
在ResNet等网络上效果更好
```

#### AlexNet中的Dropout

**应用位置**：
```
AlexNet架构：
Conv1 → Pool → Conv2 → Pool → Conv3 → Conv4 → Conv5 → Pool
→ FC6 (4096, dropout=0.5)
→ FC7 (4096, dropout=0.5)
→ FC8 (1000)
```

**效果**：
```
without Dropout：
- 训练准确率：90%
- 测试准确率：60%
- 明显过拟合

with Dropout：
- 训练准确率：85%
- 测试准确率：75%
- 泛化能力大幅提升
```

**Hinton的评价**：
> "Dropout是我职业生涯中最简单但最有效的想法之一。"

---

## 2.2 ImageNet时刻：AlexNet的横空出世（2012）

### ImageNet挑战赛

#### 数据集规模

**ImageNet ILSVRC（ImageNet Large Scale Visual Recognition Challenge）**：
```
训练集：
- 1,281,167张图像
- 1000个类别
- 每个类别约1300张图像

验证集：50,000张图像
测试集：100,000张图像

类别示例：
- 动物：金鱼、大白鲨、德国牧羊犬...
- 物体：键盘、鼠标、咖啡杯...
- 场景：海滩、森林...
```

**历史成绩**：
```
2010：
方法：传统计算机视觉（SIFT + SVM）
Top-5错误率：28.2%

2011：
方法：改进的传统方法
Top-5错误率：25.8%

年度进步：约2-3个百分点
```

### AlexNet的惊人表现

#### 基本信息

**作者**：
- Alex Krizhevsky（Hinton的博士生，第一作者）
- Ilya Sutskever（Hinton的博士后，后创立OpenAI）
- Geoffrey Hinton

**发表**：
- NeurIPS 2012
- 论文标题："ImageNet Classification with Deep Convolutional Neural Networks"

**结果**：
```
AlexNet：Top-5错误率 15.3%
第二名：Top-5错误率 26.2%

相对提升：(26.2-15.3)/26.2 ≈ 42%
绝对提升：10.9个百分点

这在计算机视觉历史上是前所未有的！
```

#### 网络架构详解

**整体结构**：
```
输入：224×224×3 RGB图像

Layer 1：卷积层
- 96个11×11×3的卷积核，步长4
- 输出：55×55×96
- ReLU激活
- 局部响应归一化（LRN）
- 最大池化（3×3，步长2）
- 输出：27×27×96

Layer 2：卷积层
- 256个5×5×48的卷积核
- 输出：27×27×256
- ReLU + LRN + 池化
- 输出：13×13×256

Layer 3：卷积层
- 384个3×3×256的卷积核
- 输出：13×13×384
- ReLU

Layer 4：卷积层
- 384个3×3×192的卷积核
- 输出：13×13×384
- ReLU

Layer 5：卷积层
- 256个3×3×192的卷积核
- 输出：13×13×256
- ReLU + 池化
- 输出：6×6×256

Layer 6：全连接层
- 4096个神经元
- ReLU + Dropout(0.5)

Layer 7：全连接层
- 4096个神经元
- ReLU + Dropout(0.5)

Layer 8：全连接层
- 1000个神经元（对应1000个类别）
- Softmax

总参数量：约6000万
```

**可视化**：
```
224×224×3
    ↓ Conv1(11×11, 96)
55×55×96
    ↓ Pool
27×27×96
    ↓ Conv2(5×5, 256)
27×27×256
    ↓ Pool
13×13×256
    ↓ Conv3(3×3, 384)
13×13×384
    ↓ Conv4(3×3, 384)
13×13×384
    ↓ Conv5(3×3, 256)
13×13×256
    ↓ Pool
6×6×256 = 9216
    ↓ FC6(4096)
4096
    ↓ FC7(4096)
4096
    ↓ FC8(1000)
1000
```

#### 创新点详解

**1. ReLU激活函数**

**首次大规模使用**：
```
之前的网络：
- LeNet：Sigmoid/Tanh
- 训练慢

AlexNet：
- 全部使用ReLU
- 训练速度提升6倍

实验对比（达到25%训练误差的时间）：
Tanh：35 epochs
ReLU：6 epochs
```

**2. 双GPU训练**

**背景**：
```
2012年的GPU：
- NVIDIA GTX 580
- 3GB显存
- 一块GPU放不下整个网络

Krizhevsky的解决方案：
- 使用2块GPU
- 网络分成两半，并行训练
- 特定层之间有通信
```

**并行方案**：
```
GPU1：处理一半卷积核
GPU2：处理另一半卷积核

通信：
- Conv2：只从同GPU的Conv1接收
- Conv3：从两个GPU的Conv2接收（全连接）
- Conv4：只从同GPU的Conv3接收
- Conv5：只从同GPU的Conv4接收
- FC层：从两个GPU接收

这种特殊设计既节省通信，又保证性能
```

**现代观点**：
- 今天的GPU显存更大（16GB、80GB）
- 不需要这种手工设计的并行
- 但开创了模型并行的先河

**3. 数据增强（Data Augmentation）**

**训练时增强**：
```python
def train_augmentation(image):
    # 1. 随机裁剪224×224（从256×256）
    image = random_crop(image, 224, 224)

    # 2. 随机水平翻转
    if random.random() < 0.5:
        image = horizontal_flip(image)

    # 3. RGB通道PCA颜色抖动
    image = pca_color_jitter(image)

    return image
```

**PCA颜色抖动详解**：
```
目的：改变颜色和光照，增加多样性

算法：
1. 对训练集RGB像素计算协方差矩阵
2. PCA分解得到主成分p₁, p₂, p₃和特征值λ₁, λ₂, λ₃
3. 对每张图像，添加扰动：
   [R, G, B] += [p₁, p₂, p₃] · [α₁λ₁, α₂λ₂, α₃λ₃]
   其中αᵢ ~ N(0, 0.1)

效果：
- 改变光照条件
- 改变颜色
- Top-1错误率降低1%以上
```

**测试时增强（Test-Time Augmentation）**：
```
1. 从256×256图像中裁剪5个224×224
   - 四角 + 中心
2. 每个裁剪的水平翻转
3. 总共10个版本
4. 10个预测的平均

这进一步降低了错误率约2%
```

**4. Dropout**

**使用位置**：
```
FC6：Dropout(0.5)
FC7：Dropout(0.5)

效果：
- 防止全连接层过拟合
- 测试错误率降低约2%
```

**5. 局部响应归一化（LRN）**

**动机**：
模拟生物神经元的侧抑制（lateral inhibition）

**公式**：
```
bⁱₓ,ᵧ = aⁱₓ,ᵧ / (k + α·Σⱼ(aʲₓ,ᵧ)²)^β

其中：
- aⁱₓ,ᵧ：位置(x,y)、通道i的激活
- 求和范围：j ∈ [max(0, i-n/2), min(N-1, i+n/2)]
- N：总通道数
- k=2, n=5, α=10⁻⁴, β=0.75（论文参数）
```

**效果**：
- Top-1和Top-5错误率各降低约1.4%和1.2%

**现代观点**：
- 2015年后，Batch Normalization替代了LRN
- LRN计算复杂，效果不如BatchNorm
- 现代网络基本不用LRN

#### 训练细节

**优化器**：
```
算法：带动量的SGD

momentum = 0.9
weight_decay = 0.0005

更新规则：
vᵢ₊₁ = 0.9·vᵢ - 0.0005·ϵ·wᵢ - ϵ·⟨∂L/∂w|wᵢ⟩
wᵢ₊₁ = wᵢ + vᵢ₊₁

其中：
- vᵢ：动量
- ϵ：学习率
- ⟨∂L/∂w|wᵢ⟩：批次梯度的平均
```

**学习率调度**：
```
初始学习率：0.01

策略：当验证错误率不再下降时，手动降低10倍

实际：
- 训练了90个epochs
- 学习率降低了3次
```

**批次大小**：
```
Batch size：128

设备：
- 2块NVIDIA GTX 580 GPU
- 每块GPU处理64个样本
```

**初始化**：
```
权重：
- 均值0、标准差0.01的高斯分布

偏置：
- Conv2, Conv4, Conv5和所有FC层：初始化为1
- 其他层：初始化为0

原因：
- 正偏置为ReLU提供正输入，加速早期学习
```

**训练时间**：
```
硬件：
- 2块NVIDIA GTX 580 GPU（3GB显存）

时间：
- 5-6天
- 90个epochs

对比今天：
- 使用V100或A100 GPU
- 可能只需数小时到1天
```

#### 可视化学习到的特征

**第一层卷积核（11×11×3 → 96）**：
```
学到的模式：
- 颜色块（不同颜色的响应）
- 边缘检测器（不同方向）
- Gabor滤波器（纹理）

GPU1的核：
- 主要是颜色无关的特征

GPU2的核：
- 主要是颜色相关的特征

这种分工是自动出现的！
```

**高层特征可视化**：
```
方法：
找到最大激活某个神经元的图像块

发现：
- Conv5：学到物体部件
  （人脸、文字、动物纹理）
- FC6：学到更抽象的模式
- FC7：学到语义概念
```

#### AlexNet的影响

**1. 学术界**
```
- 证明了深度学习在大规模视觉任务上的威力
- 引发深度学习研究热潮
- NeurIPS 2012论文被引用超过10万次

论文引用趋势：
2012：几百次
2015：数千次
2020：数万次
2024：超过10万次
```

**2. 工业界**
```
- Google、Facebook、Microsoft等开始大规模投入
- GPU成为AI训练的标配
- 创业公司如雨后春笋

投资趋势：
2012年前：AI寒冬，投资谨慎
2012年后：AI复兴，投资爆发

NVIDIA股价：
2012：~$13
2024：~$800+（拆股调整后）
```

**3. ImageNet竞赛**
```
2012：AlexNet 15.3%
2013：ZFNet 11.2%（深度学习）
2014：GoogLeNet 6.7%（深度学习）
2015：ResNet 3.57%（深度学习）
2017：竞赛停止（已超过人类水平）

从2012年起，深度学习统治ImageNet
```

**4. 工具和生态**
```
AlexNet后的发展：
- Caffe框架（2013，伯克利）：AlexNet的官方实现
- TensorFlow（2015，Google）
- PyTorch（2016，Facebook）

这些工具让深度学习普及化
```

**5. 应用爆发**
```
2012年后，深度学习应用于：
- 人脸识别（Face++、商汤）
- 自动驾驶（Waymo、Tesla）
- 医疗影像（诊断肺癌、糖尿病视网膜病变）
- 安防监控
- 手机相机（计算摄影）
```

---

## 2.3 深度学习的黄金时代（2013-2017）

### 计算机视觉的飞速进步

#### 2014年：VGGNet - 更深的网络

**作者**：
牛津大学视觉几何组（Visual Geometry Group）
- Karen Simonyan
- Andrew Zisserman

**核心哲学**：
> "深度是关键，结构越简单越好"

**架构特点**：

**统一使用3×3卷积**：
```
为什么用3×3？

1. 感受野叠加：
   - 2个3×3卷积 = 1个5×5感受野
   - 3个3×3卷积 = 1个7×7感受野

2. 参数更少：
   - 1个7×7卷积：7×7×C² = 49C²参数
   - 3个3×3卷积：3×(3×3×C²) = 27C²参数
   - 节省45%参数

3. 更多非线性：
   - 3个ReLU vs 1个ReLU
   - 表达能力更强
```

**VGG-16架构**：
```
输入：224×224×3

Block 1：
Conv3-64 → Conv3-64 → MaxPool
输出：112×112×64

Block 2：
Conv3-128 → Conv3-128 → MaxPool
输出：56×56×128

Block 3：
Conv3-256 → Conv3-256 → Conv3-256 → MaxPool
输出：28×28×256

Block 4：
Conv3-512 → Conv3-512 → Conv3-512 → MaxPool
输出：14×14×512

Block 5：
Conv3-512 → Conv3-512 → Conv3-512 → MaxPool
输出：7×7×512

FC层：
FC-4096 → FC-4096 → FC-1000

总层数：
13个卷积层 + 3个全连接层 = 16层
```

**VGG-19**：
- 在Block 3, 4, 5中各增加一个卷积层
- 总共19层

**参数量**：
```
VGG-16：约1.38亿参数
VGG-19：约1.44亿参数

大部分在FC层：
Conv层：~1400万参数
FC层：~1.24亿参数（约90%）
```

**性能**：
```
ImageNet Top-5错误率：
VGG-16：7.4%
VGG-19：7.3%

相比AlexNet（15.3%），大幅提升
```

**优点**：
```
1. 结构简单统一
   - 只用3×3卷积和2×2池化
   - 易于理解和实现

2. 迁移学习效果好
   - VGG特征在其他任务上表现优异
   - 成为特征提取的标准网络

3. 验证了深度的重要性
   - 越深性能越好（16层 > 11层）
```

**缺点**：
```
1. 参数量巨大
   - 内存占用高
   - 训练慢

2. 推理慢
   - 计算量大（约150亿次浮点运算）

3. FC层参数冗余
   - 90%参数在FC层
   - 后来被全局平均池化替代
```

**遗产**：
- 3×3卷积成为标准
- 证明深度的重要性
- 预训练模型广泛使用

---

#### 2014年：GoogLeNet - Inception模块

**作者**：
Google研究团队
- Christian Szegedy等

**核心思想**：
> "不同尺度的特征同时捕获"

**Inception模块**：

**动机**：
```
问题：
- 应该用1×1、3×3还是5×5卷积？
- 应该用卷积还是池化？

传统方法：
- 人工选择一种

Inception的方案：
- 全部都用！并行计算，拼接结果
```

**Inception v1结构**：
```
输入
├─ 1×1 卷积
├─ 1×1 卷积 → 3×3 卷积
├─ 1×1 卷积 → 5×5 卷积
└─ 3×3 MaxPool → 1×1 卷积
    ↓
  拼接(Concatenate)
    ↓
  输出
```

**1×1卷积的作用**：
```
1. 降维（减少计算量）：
   例子：
   输入：28×28×192
   直接5×5卷积到32通道：
   → 参数：5×5×192×32 = 153,600

   先1×1降到16通道，再5×5到32通道：
   → 参数：1×1×192×16 + 5×5×16×32 = 15,872
   → 节省90%参数！

2. 增加非线性：
   - 1×1卷积后有ReLU
   - 更多非线性变换
```

**完整Inception模块（带降维）**：
```
输入（28×28×192）
├─ 1×1 Conv(64)  →  28×28×64
├─ 1×1 Conv(96)  →  3×3 Conv(128)  →  28×28×128
├─ 1×1 Conv(16)  →  5×5 Conv(32)   →  28×28×32
└─ 3×3 MaxPool   →  1×1 Conv(32)   →  28×28×32
    ↓
  Concat  →  28×28×(64+128+32+32) = 28×28×256
```

**GoogLeNet整体架构**：
```
输入：224×224×3

初始卷积：
Conv7×7(64, stride=2) → MaxPool → Conv1×1(64) → Conv3×3(192) → MaxPool

Inception模块 ×9：
Inception(3a) → Inception(3b) → MaxPool
Inception(4a) → Inception(4b) → Inception(4c) → Inception(4d) → Inception(4e) → MaxPool
Inception(5a) → Inception(5b)

全局平均池化 → Dropout → FC(1000) → Softmax

总深度：22层（带参数的层）
```

**辅助分类器（Auxiliary Classifiers）**：

**动机**：
- 网络很深，梯度消失问题
- 需要中间层也能得到监督信号

**设计**：
```
在Inception(4a)和Inception(4d)后添加：
AvgPool → Conv1×1 → FC → FC → Softmax

训练时：
总损失 = 主损失 + 0.3×辅助损失1 + 0.3×辅助损失2

测试时：
只用主分类器，忽略辅助分类器
```

**效果**：
- 加速收敛
- 提供额外的梯度信号
- 正则化作用

**参数量对比**：
```
AlexNet：6000万参数
VGGNet：1.38亿参数
GoogLeNet：700万参数

GoogLeNet比VGGNet：
- 参数少20倍
- 计算量少3倍
- 性能更好
```

**ImageNet成绩**：
```
Top-5错误率：6.7%
赢得ILSVRC 2014冠军
```

**后续版本**：
```
Inception v2 (2015)：
- 用两个3×3替代5×5
- Batch Normalization

Inception v3 (2015)：
- 分解卷积（如3×3 → 1×3和3×1）
- 更深（42层）

Inception v4 (2016)：
- 结合ResNet的残差连接
- Inception-ResNet
```

**核心贡献**：
```
1. Inception模块：
   - 多尺度特征并行捕获
   - 1×1卷积降维

2. 全局平均池化：
   - 替代全连接层
   - 减少参数

3. 辅助分类器：
   - 缓解梯度消失

4. 证明"宽度"也重要：
   - 不只是深度，宽度（通道数）也很重要
```

---

#### 2015年：ResNet - 残差连接

**作者**：
微软亚洲研究院（MSRA）
- 何恺明（Kaiming He，第一作者）
- 张翔宇
- 任少卿
- 孙剑

**背景问题**：

**深度的悖论**：
```
直觉：
网络越深 → 表达能力越强 → 性能应该越好

实际观察：
VGG-16：7.4% 错误率
VGG-19：7.3%（略好）
Plain-20：更差
Plain-56：更差！（不是过拟合，训练误差也更高）

反直觉现象：
56层网络性能 < 20层网络
```

**退化问题（Degradation Problem）**：
```
不是过拟合（训练误差也高）
不是梯度消失（用了BN）

问题：
深层网络难以学习恒等映射
如果深层是"冗余的"，
至少应该学会恒等映射（复制浅层的性能）
但实际上没有
```

**残差连接的突破**

**核心思想**：
```
传统：
学习映射 H(x) = 目标输出

ResNet：
学习残差 F(x) = H(x) - x
→ H(x) = F(x) + x
```

**残差块（Residual Block）**：
```
输入 x
├─ 权重层1
├─ ReLU
├─ 权重层2
└─ (学习F(x))
   ↓
   + ← x（跳跃连接/shortcut）
   ↓
  ReLU
   ↓
  输出
```

**数学表达**：
```
y = F(x, {Wᵢ}) + x

其中：
- x：输入
- F(x, {Wᵢ})：残差函数（需要学习）
- +：逐元素相加
```

**为什么有效？**

**1. 恒等映射容易学习**
```
目标：H(x) = x（恒等映射）

传统网络：
需要学习H(x) = x
→ 所有权重学成单位矩阵
→ 困难

ResNet：
需要学习F(x) = 0
→ 所有权重学成0
→ 简单！（权重衰减自然推向0）
```

**2. 梯度高速公路**
```
反向传播：
∂L/∂x = ∂L/∂y · ∂y/∂x
       = ∂L/∂y · ∂(F(x)+x)/∂x
       = ∂L/∂y · (∂F(x)/∂x + 1)

关键：
- 有一个常数项"1"
- 梯度可以"直达"前面的层
- 即使∂F(x)/∂x很小，梯度也不会消失

类比：
高速公路（直接通过） + 辅路（局部调整）
```

**3. 集成效应**
```
理论研究表明：
ResNet可以看作多个浅层网络的集成

n个残差块：
→ 2ⁿ条路径（每个块可以走或不走）

例：
3个残差块
→ 2³=8条路径
→ 相当于8个不同深度网络的集成
```

**ResNet架构系列**

**基本残差块**（用于ResNet-34及更浅）：
```
输入
└─ 3×3 Conv
   └─ BatchNorm
      └─ ReLU
         └─ 3×3 Conv
            └─ BatchNorm
               ↓
             + ← 输入
               ↓
             ReLU
```

**瓶颈残差块**（Bottleneck，用于ResNet-50及更深）：
```
输入
└─ 1×1 Conv（降维，如256→64）
   └─ BatchNorm → ReLU
      └─ 3×3 Conv（64→64）
         └─ BatchNorm → ReLU
            └─ 1×1 Conv（升维，64→256）
               └─ BatchNorm
                  ↓
                + ← 输入
                  ↓
                ReLU

目的：
- 降低计算量
- 1×1降维 → 3×3计算 → 1×1升维
- 类似Inception的思想
```

**ResNet-50架构**：
```
输入：224×224×3

Conv1：
7×7 Conv(64, stride=2) → BatchNorm → ReLU → MaxPool(3×3, stride=2)
输出：56×56×64

Conv2_x（56×56）：
[1×1 Conv(64) → 3×3 Conv(64) → 1×1 Conv(256)] × 3个残差块
输出：56×56×256

Conv3_x（28×28）：
[1×1 Conv(128) → 3×3 Conv(128) → 1×1 Conv(512)] × 4个残差块
输出：28×28×512

Conv4_x（14×14）：
[1×1 Conv(256) → 3×3 Conv(256) → 1×1 Conv(1024)] × 6个残差块
输出：14×14×1024

Conv5_x（7×7）：
[1×1 Conv(512) → 3×3 Conv(512) → 1×1 Conv(2048)] × 3个残差块
输出：7×7×2048

全局平均池化 → FC(1000)

总层数：
1 + (3+4+6+3)×3 + 1 = 50层
```

**ResNet系列对比**：
```
ResNet-18：18层，基本块
ResNet-34：34层，基本块
ResNet-50：50层，瓶颈块
ResNet-101：101层，瓶颈块
ResNet-152：152层，瓶颈块
ResNet-1202：1202层（研究用）
```

**ImageNet结果**：
```
ResNet-50：5.25% Top-5错误率
ResNet-101：4.60%
ResNet-152：3.57%

ResNet-152性能：
- 超过人类水平（~5%）
- ILSVRC 2015冠军
```

**极深网络的实验**：
```
ResNet-1202（1202层）：
- 可以收敛（梯度不消失）
- 但性能不如ResNet-110

原因：
- 训练集不够大
- 过拟合
- 说明深度不是唯一因素
```

**维度不匹配的处理**：
```
问题：
x的通道数 ≠ F(x)的通道数
→ 无法相加

解决方案：

方案A：零填充
如x是64通道，F(x)是128通道
→ 在x后面补64个0通道
优点：无参数
缺点：信息损失

方案B：投影（1×1卷积）
x' = W·x（1×1卷积）
→ y = F(x) + x'
优点：信息无损失
缺点：增加参数

实践：
- 通道数不变时：恒等映射
- 通道数翻倍时：方案B（投影）
```

**ResNet的影响**

**1. 理论启示**：
```
- 深度网络的优化是核心问题
- 好的架构设计可以改变优化景观
- 恒等映射的重要性
```

**2. 实践影响**：
```
- 成为标准架构（ResNet-50是最常用的骨干网络）
- 几乎所有后续网络都有跳跃连接
- 预训练模型广泛使用
```

**3. 后续发展**：
```
ResNeXt（2017）：
- 增加"基数"（cardinality）维度
- 多个并行路径

WideResNet（2016）：
- 增加宽度（通道数）
- 减少深度

DenseNet（2017）：
- 每层连接到所有后续层
- 更密集的连接

Res2Net, ResNeSt等变体
```

**何恺明的后续工作**：
```
- Faster R-CNN（目标检测）
- Mask R-CNN（实例分割）
- Focal Loss（目标检测）
- Momentum Contrast（自监督学习）

2022年：离开Meta，加入MIT
被认为是计算机视觉领域最有影响力的研究者之一
```

---

### 生成模型的崛起

#### 2014年：GAN（生成对抗网络）- Ian Goodfellow

**诞生的传说**：
```
2014年某天晚上，蒙特利尔酒吧
Ian Goodfellow与朋友讨论生成模型

朋友的问题：
"如何让机器生成逼真的图像？"

当时的方法：
- VAE（变分自编码器）：图像模糊
- 显式密度建模：计算困难

Goodfellow的灵感：
"让两个神经网络对抗！"

当晚回家，编码实现
第一次运行就成功了！

2014年6月：论文提交
现在：被引用超过8万次
```

**核心思想**

**对抗博弈**：
```
生成器G（造假者）：
- 从噪声生成假样本
- 目标：骗过判别器

判别器D（鉴别家）：
- 区分真样本和假样本
- 目标：准确判断

训练过程：
G和D相互对抗，同时提升
最终G生成的样本D无法区分
```

**类比**：
```
造假币的故事：
- 造假者（G）：制造假币
- 警察（D）：鉴别真假币

开始：
- 造假者技术差，警察轻松识别

过程：
- 造假者改进技术 → 警察提升鉴别能力
- 造假者再改进 → 警察再提升
- ...循环对抗

结果：
- 造假者技术精湛，假币几乎完美
- 警察也练就火眼金睛
```

**数学表达式**

**目标函数**：
```
min_G max_D V(D,G) = E_x~p_data[log D(x)] + E_z~p_z[log(1-D(G(z)))]

分解：
判别器D的目标（max）：
V(D,G) = E_x~p_data[log D(x)] + E_z~p_z[log(1-D(G(z)))]
          真样本打高分          假样本打低分

生成器G的目标（min）：
V(D,G) = E_z~p_z[log(1-D(G(z)))]
         让假样本骗过D

其中：
- x：真实数据
- z：噪声（通常是高斯分布）
- G(z)：生成的假样本
- D(x)：判别x为真的概率
- p_data：真实数据分布
- p_z：噪声分布
```

**直觉理解**：
```
判别器D：
log D(x)：x是真的，希望D(x)→1，log D(x)→0
log(1-D(G(z)))：G(z)是假的，希望D(G(z))→0，log(1-D(G(z)))→0

生成器G：
log(1-D(G(z)))：希望D被骗，D(G(z))→1，log(1-D(G(z)))→-∞
（所以G要minimize）
```

**训练算法**

**标准GAN训练流程**：
```python
for epoch in range(num_epochs):
    for real_batch in dataloader:
        # 训练判别器D
        noise = sample_noise(batch_size)
        fake_batch = G(noise)

        d_loss = -log(D(real_batch)) - log(1 - D(fake_batch))
        D.zero_grad()
        d_loss.backward()
        optimizer_D.step()

        # 训练生成器G
        noise = sample_noise(batch_size)
        fake_batch = G(noise)

        g_loss = -log(D(fake_batch))  # 希望D(fake)→1
        G.zero_grad()
        g_loss.backward()
        optimizer_G.step()
```

**改进：Non-saturating GAN**
```
问题：
早期训练时，D很强，D(G(z))≈0
→ log(1-D(G(z)))≈0
→ G的梯度很小，学习慢

改进：
G的目标改为 max log D(G(z))
（等价于min -log D(G(z))）

效果：
- 即使D(G(z))很小，梯度也不会消失
- 训练更稳定
```

**网络架构（DCGAN，2015）**

**生成器G**：
```
输入：噪声z（如100维）

FC → Reshape → 4×4×1024

DeConv(512) → BatchNorm → ReLU → 8×8×512
DeConv(256) → BatchNorm → ReLU → 16×16×256
DeConv(128) → BatchNorm → ReLU → 32×32×128
DeConv(3)   → Tanh → 64×64×3

关键设计：
- 全卷积（无FC层）
- BatchNorm（除输出层）
- ReLU（生成器），Leaky ReLU（判别器）
- Tanh输出（归一化到[-1,1]）
```

**判别器D**：
```
输入：图像（64×64×3）

Conv(128) → LeakyReLU → 32×32×128
Conv(256) → BatchNorm → LeakyReLU → 16×16×256
Conv(512) → BatchNorm → LeakyReLU → 8×8×512
Conv(1024) → BatchNorm → LeakyReLU → 4×4×1024

Flatten → FC(1) → Sigmoid

关键设计：
- 全卷积
- BatchNorm（除输入层）
- LeakyReLU（斜率0.2）
- 步长卷积替代池化
```

**GAN的优势**

```
1. 生成质量高：
   - 图像锐利清晰
   - 不像VAE那样模糊

2. 无需显式密度建模：
   - 不需要计算似然
   - 计算高效

3. 灵活性强：
   - G和D可以是任意神经网络
   - 可以生成各种模态（图像、文本、音频）
```

**GAN的挑战**

**1. 训练不稳定**
```
问题：
- 模式崩溃（Mode Collapse）：G生成单一样本
- 梯度消失：D太强，G学不到
- 不收敛：G和D震荡

缓解：
- 仔细调整学习率
- 使用WGAN等改进算法
- 谱归一化（Spectral Normalization）
```

**2. 模式崩溃（Mode Collapse）**
```
现象：
G只生成几种样本，忽略数据分布的多样性

例子：
训练生成手写数字
理想：生成0-9各种数字
实际：只生成少数几个数字（如只有1和7）

原因：
- G找到能骗过D的简单策略
- 缺乏多样性惩罚

缓解：
- Unrolled GAN
- Minibatch Discrimination
- Experience Replay
```

**3. 评估困难**
```
问题：
如何评估生成质量？

传统指标（似然）：
GAN不适用（无显式密度）

替代指标：
- Inception Score（IS）
- Fréchet Inception Distance（FID）
- 人工评估

但仍不完美
```

**GAN的应用**

**1. 图像生成**
```
StyleGAN（2018-2020）：
- 生成高分辨率人脸（1024×1024）
- ThisPersonDoesNotExist.com
- 逼真到无法区分

BigGAN（2018）：
- ImageNet级别的图像生成
- 512×512高分辨率
```

**2. 图像翻译**
```
Pix2Pix（2017）：
- 成对图像翻译
- 草图→照片、黑白→彩色

CycleGAN（2017）：
- 无需成对数据
- 马→斑马、夏天→冬天
- 照片→名画风格
```

**3. 超分辨率**
```
SRGAN（2017）：
- 低分辨率→高分辨率
- 清晰度超过传统方法
```

**4. 数据增强**
```
生成额外训练数据：
- 医疗图像（数据稀缺）
- 少样本学习
```

**5. 其他应用**
```
- 视频生成
- 文本生成（SeqGAN）
- 音乐生成
- 药物分子设计
```

**GAN的演变**

```
2014：原始GAN（Goodfellow）
2015：DCGAN（稳定的卷积GAN）
2016：InfoGAN、f-GAN
2017：WGAN（Wasserstein GAN，改进训练）、Pix2Pix、CycleGAN
2018：BigGAN、StyleGAN、Progressive GAN
2019：StyleGAN2
2020：StyleGAN2-ADA
2021-2024：扩散模型逐渐取代GAN成为生成模型主流
```

**Yann LeCun的评价**：
> "GAN是机器学习近10年来最有趣的想法"

---

#### 2013年：VAE（变分自编码器）

**作者**：
- Diederik P. Kingma
- Max Welling

**核心思想**：
```
学习数据的潜在表示（latent representation）
同时能生成新样本
```

**编码器-解码器结构**：
```
编码器：x → z
将数据x编码为潜在变量z

解码器：z → x'
从潜在变量z重构数据x'
```

**与传统自编码器的区别**：
```
传统自编码器：
x → Encoder → z → Decoder → x'
潜在变量z是确定的

VAE：
x → Encoder → μ(x), σ(x) → z ~ N(μ, σ²) → Decoder → x'
潜在变量z是随机的，服从分布
```

**目标函数**：
```
ELBO（Evidence Lower BOund）：
L = E_z[log p(x|z)] - KL(q(z|x) || p(z))
    重构损失           正则化项

重构损失：
希望解码器能重构原始数据

KL散度：
希望编码的分布q(z|x)接近先验p(z)（通常是N(0,I)）
```

**重参数化技巧（Reparameterization Trick）**：
```
问题：
z = μ + σ·ε，其中ε ~ N(0,1)
如何对随机采样求导？

解决：
将随机性转移到ε
z = μ + σ·ε
对μ和σ求导（ε是常数）
```

**VAE vs GAN**：
```
VAE优点：
- 训练稳定
- 有明确的优化目标
- 可以编码（给定x，得到z）

VAE缺点：
- 生成图像模糊
- 最大似然目标不够好

GAN优点：
- 生成图像清晰
- 质量高

GAN缺点：
- 训练不稳定
- 无法编码

结论：
两者互补，后来有VAE-GAN结合
```

**应用**：
```
- 图像生成（但不如GAN清晰）
- 异常检测（重构误差大→异常）
- 数据压缩
- 半监督学习
```

---

## 2.4 NLP的深度学习转型（2013-2017）

### Word2Vec：词语的几何学（2013）

#### 传统词表示的问题

**One-hot编码**：
```
词汇表：["cat", "dog", "king", "queen", "apple"]

"cat":   [1, 0, 0, 0, 0]
"dog":   [0, 1, 0, 0, 0]
"king":  [0, 0, 1, 0, 0]
"queen": [0, 0, 0, 1, 0]
"apple": [0, 0, 0, 0, 1]

问题：
1. 维度灾难：
   词汇量N → N维向量
   N=10万 → 10万维

2. 稀疏性：
   只有一个1，其余全是0
   计算和存储低效

3. 无语义：
   "cat"和"dog"的距离 = "cat"和"apple"的距离
   无法表达语义相似性
```

**分布式表示的想法**：
```
假设：
"一个词的意义由其上下文决定"
（Distributional Hypothesis，Firth 1957）

例子：
"I have a cute ___"
空格处可能是：cat, dog, rabbit
→ 这些词语义相近

如果两个词经常出现在相似的上下文：
→ 它们语义相近
→ 应该有相近的向量表示
```

#### Word2Vec的两种架构

**1. CBOW（Continuous Bag of Words）**

**任务**：
```
给定上下文，预测中心词

例子：
上下文：["I", "have", "a", "cute"]
中心词："cat"
```

**网络结构**：
```
输入：上下文词的one-hot编码
w_{t-2}, w_{t-1}, w_{t+1}, w_{t+2}

嵌入层（共享权重矩阵W）：
v_{t-2} = W·w_{t-2}
v_{t-1} = W·w_{t-1}
v_{t+1} = W·w_{t+1}
v_{t+2} = W·w_{t+2}

平均：
v = (v_{t-2} + v_{t-1} + v_{t+1} + v_{t+2}) / 4

输出层（权重矩阵W'）：
score = W'·v
p(w_t | context) = softmax(score)
```

**训练目标**：
```
最大化：
log p(w_t | w_{t-c}, ..., w_{t-1}, w_{t+1}, ..., w_{t+c})

其中c是窗口大小
```

**2. Skip-gram**

**任务**：
```
给定中心词，预测上下文

例子：
中心词："cat"
上下文：["I", "have", "a", "cute"]
```

**网络结构**：
```
输入：中心词w_t的one-hot编码

嵌入层：
v_t = W·w_t

对每个上下文词w_c：
score_c = W'·v_t
p(w_c | w_t) = softmax(score_c)
```

**训练目标**：
```
最大化：
Σ_{-c≤j≤c, j≠0} log p(w_{t+j} | w_t)

假设上下文词独立给定中心词
```

**CBOW vs Skip-gram**：
```
CBOW：
- 上下文 → 中心词
- 训练快（一次预测）
- 适合小数据

Skip-gram：
- 中心词 → 上下文
- 训练慢（预测多个词）
- 对罕见词效果好
- 实践中更常用
```

#### 训练加速技巧

**问题**：
```
Softmax计算：
p(w_O | w_I) = exp(v'_{w_O}·v_{w_I}) / Σ_{w=1}^V exp(v'_w·v_{w_I})

分母需要遍历整个词汇表V
V=10万 → 每次预测计算10万次
太慢！
```

**解决方案1：层次Softmax（Hierarchical Softmax）**
```
将词汇表组织成二叉树（Huffman树）：
- 叶节点：词
- 路径：从根到叶的路径

预测：
不是计算V个概率，
而是沿着树路径做log V次二分类

复杂度：
O(V) → O(log V)
```

**解决方案2：负采样（Negative Sampling）**

**核心思想**：
```
不计算完整softmax，
只区分正样本和负样本

正样本：
(中心词, 真实上下文词)

负样本：
(中心词, 随机采样的词)
```

**目标函数**：
```
对每个正样本(w, c)：
log σ(v'_c·v_w) + Σ_{i=1}^k E_{w_i~P_n}[log σ(-v'_{w_i}·v_w)]

第一项：正样本得分高
第二项：k个负样本得分低

其中：
- σ：sigmoid函数
- k：负样本数量（通常5-20）
- P_n：负采样分布（通常是词频的3/4次方）
```

**负采样分布**：
```
为什么用f(w)^{3/4}？

直觉：
- 高频词：稍微降低采样概率
- 低频词：稍微提升采样概率
- 更平衡

例子：
"the"：f("the")=0.1
"aardvark"：f("aardvark")=0.0001

均匀采样：
"the"采样概率=0.0001（太少）

词频采样：
"the"采样概率=0.1（太多）

f^{3/4}：
"the"采样概率 ≈ 0.056（适中）
```

**复杂度**：
```
层次Softmax：O(log V)
负采样：O(k)，k通常5-20

实践中，负采样更快
```

#### 神奇的词向量运算

**著名的"国王-男人+女人=女王"**：
```
vec("king") - vec("man") + vec("woman") ≈ vec("queen")

验证：
cos_sim(vec("king") - vec("man") + vec("woman"), vec("queen")) ≈ 0.8+
```

**其他例子**：
```
vec("Paris") - vec("France") + vec("Italy") ≈ vec("Rome")
（首都关系）

vec("big") - vec("bigger") + vec("small") ≈ vec("smaller")
（语法关系）

vec("walked") - vec("walk") + vec("swim") ≈ vec("swam")
（时态关系）
```

**为什么会这样？**

**线性关系的出现**：
```
观察：
vec("man") - vec("woman") ≈ vec("king") - vec("queen")
→ 性别差异是一个固定的向量方向

原因：
- Word2Vec优化目标隐式地保留了词之间的关系
- 上下文相似的词，向量相似
- "king"和"queen"的上下文相似（除了性别）
- 性别差异被编码为向量空间的一个方向
```

**可视化（t-SNE降维到2D）**：
```
                   king·  ·queen
                        \/
               man·        ·woman

             uncle·    ·aunt

           doctor·    ·nurse (性别偏见！)
```

#### Word2Vec的影响

**1. 成为NLP的基础**：
```
2013年后几乎所有NLP任务：
- 输入：词向量（而非one-hot）
- Word2Vec预训练，下游任务微调
```

**2. 迁移学习的雏形**：
```
预训练（大规模无标注文本）：
- 学习词向量

微调（具体任务）：
- 使用预训练词向量初始化

这是BERT、GPT等预训练模型的先驱
```

**3. 启发了其他工作**：
```
GloVe（2014，Stanford）：
- 基于全局词共现矩阵
- 结合了矩阵分解和Word2Vec

FastText（2016，Facebook）：
- 考虑子词（subword）信息
- 对罕见词、拼写错误更鲁棒

ELMo（2018）：
- 上下文词向量
- 不同上下文中，同一个词的向量不同
```

**4. 揭示了偏见问题**：
```
发现：
vec("doctor") - vec("man") + vec("woman") ≈ vec("nurse")
vec("programmer") - vec("man") + vec("woman") ≈ vec("homemaker")

原因：
训练语料反映了社会偏见

后果：
下游应用可能放大偏见（如简历筛选）

解决：
- 去偏见算法（投影、对抗训练）
- 但完全消除很困难
```

#### 实现细节

**Gensim实现示例**：
```python
from gensim.models import Word2Vec

# 训练数据：句子列表
sentences = [
    ["I", "love", "deep", "learning"],
    ["Deep", "learning", "is", "amazing"],
    ...
]

# 训练模型
model = Word2Vec(
    sentences=sentences,
    vector_size=100,      # 词向量维度
    window=5,             # 上下文窗口
    min_count=5,          # 最小词频
    sg=1,                 # 1=Skip-gram, 0=CBOW
    negative=5,           # 负采样数量
    workers=4             # 并行线程数
)

# 使用
vec_king = model.wv['king']
similar_words = model.wv.most_similar('king', topn=10)
result = model.wv.most_similar(positive=['king', 'woman'],
                                negative=['man'], topn=1)
```

**训练语料**：
```
常用数据集：
- Google News（100B词）
- Wikipedia dump
- Common Crawl

预训练模型：
可以直接下载使用（如gensim提供的模型）
```

---

### Seq2Seq与注意力机制（2014-2015）

#### Seq2Seq架构（2014）

**作者**：
- Ilya Sutskever（Google，OpenAI联合创始人）
- Oriol Vinyals
- Quoc Le

**论文**：
"Sequence to Sequence Learning with Neural Networks"

**核心思想**：
```
用两个RNN：
- 编码器（Encoder）：将输入序列编码为固定长度向量
- 解码器（Decoder）：从向量生成输出序列
```

**架构**：
```
输入序列：x₁, x₂, ..., x_T

编码器：
h₁ = RNN(x₁, h₀)
h₂ = RNN(x₂, h₁)
...
h_T = RNN(x_T, h_{T-1})

上下文向量：
c = h_T （编码器最后状态）

解码器：
s₁ = RNN(c, <START>)
s₂ = RNN(s₁, y₁)
...
s_T' = RNN(s_{T'-1}, y_{T'-1})

输出：
y_t = softmax(W·s_t)
```

**示例：机器翻译**：
```
输入（英语）：["How", "are", "you", "?"]
输出（法语）：["Comment", "allez", "vous", "?"]

编码阶段：
"How"  → h₁
"are"  → h₂
"you"  → h₃
"?"    → h₄ = c（上下文向量）

解码阶段：
c, <START> → "Comment"
s₁, "Comment" → "allez"
s₂, "allez" → "vous"
s₃, "vous" → "?"
s₄, "?" → <END>
```

**关键技巧**：

**1. 反转输入序列**：
```
不反转：
输入：["How", "are", "you"]
编码后：h_3包含"How"的信息很少（经过3步传播）

反转：
输入：["you", "are", "How"]
编码后：h_3包含"How"的信息较多（只经过1步）

效果：
BLEU分数提升约5个点
```

**2. 深层LSTM**：
```
使用4层LSTM：
- 编码器：4层
- 解码器：4层

每层1000个隐藏单元
总参数：约380M
```

**3. Beam Search解码**：
```
问题：
贪心解码（每步选概率最高的词）可能次优

例子：
- 贪心："I am very very happy" (P=0.01)
- 更好："I am extremely happy" (P=0.02)

Beam Search：
- 维护k个候选序列（beam size）
- 每步扩展每个候选的top-k词
- 保留总概率最高的k个

k=1：贪心搜索
k=5-10：常用设置
k越大：质量越好，但速度越慢
```

**问题：瓶颈在上下文向量**

```
上下文向量c是固定长度（如1000维）
需要编码整个输入序列的所有信息

问题：
- 长序列：信息损失
- "How are you" → c (容易)
- 100词的段落 → c (困难！)

瓶颈：
所有信息压缩到固定长度向量
```

#### 注意力机制的救赎（2015）

**作者**：
- Dzmitry Bahdanau（Yoshua Bengio的学生）
- Kyunghyun Cho
- Yoshua Bengio

**论文**：
"Neural Machine Translation by Jointly Learning to Align and Translate"

**核心思想**：
```
不要把所有信息压缩到一个向量！
解码时，动态关注编码器的不同位置
```

**注意力机制**：
```
编码器输出：
h₁, h₂, ..., h_T （每个时间步的隐藏状态）

解码器在时间步t：
不只看c，而是计算加权平均c_t

1. 计算注意力分数：
   e_{t,i} = score(s_{t-1}, h_i)
   （解码器状态s_{t-1}与编码器状态h_i的相关性）

2. 归一化（softmax）：
   α_{t,i} = exp(e_{t,i}) / Σ_j exp(e_{t,j})
   （注意力权重）

3. 加权平均：
   c_t = Σ_i α_{t,i}·h_i
   （动态上下文向量）

4. 解码：
   s_t = RNN([c_t, y_{t-1}], s_{t-1})
   y_t = softmax(W·s_t)
```

**注意力分数的计算**：
```
方法1：点积（Dot Product）
score(s, h) = s^T·h

方法2：加性（Additive）
score(s, h) = v^T·tanh(W₁·s + W₂·h)

方法3：乘性（Multiplicative）
score(s, h) = s^T·W·h

Bahdanau使用方法2（加性）
后来Luong等人（2015）提出方法1和3
```

**可视化注意力**：

```
翻译："The agreement on the European Economic Area was signed in August 1992"
到法语："L'accord sur la zone économique européenne a été signé en août 1992"

注意力矩阵（解码词 × 编码词）：
              The agree  on   the  Euro Econ Area was signed in  Aug  1992
L'           0.9 0.0   0.0  0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  0.0
accord       0.0 0.8   0.1  0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  0.0
sur          0.0 0.0   0.9  0.0  0.0  0.0  0.0  0.0  0.0    0.0  0.0  0.0
la           0.0 0.0   0.0  0.8  0.0  0.0  0.0  0.0  0.0    0.0  0.0  0.0
zone         0.0 0.0   0.0  0.0  0.0  0.0  0.9  0.0  0.0    0.0  0.0  0.0
...

观察：
- 大部分是对角线（单调对齐）
- "European Economic Area" → "zone économique européenne"（顺序不同）
  注意力自动学会了重排序！
```

**注意力的优势**：

```
1. 解决长序列问题：
   - 不需要压缩到固定向量
   - 直接访问编码器所有状态

2. 可解释性：
   - 可视化注意力权重
   - 看到模型"关注"哪些词

3. 性能提升：
   - BLEU分数提升2-5个点
   - 对长句子效果尤其明显
```

**应用**：
```
机器翻译（2016年后）：
- Google Translate切换到NMT（神经机器翻译）
- 使用注意力机制
- 翻译质量大幅提升

其他任务：
- 图像描述生成（注意图像不同区域）
- 语音识别（注意音频不同帧）
- 文本摘要
- 问答系统
```

**注意力机制的演变**：
```
2015：Bahdanau Attention（加性）
2015：Luong Attention（点积、乘性）
2017：Self-Attention（Transformer）
      → 革命性突破！
```

---

## 本章总结

2012-2017年，深度学习从学术研究走向实际应用，取得了一系列突破性进展：

**计算机视觉**：
- 2012：AlexNet - 证明深度学习的威力
- 2014：VGGNet - 证明深度的重要性
- 2014：GoogLeNet - Inception模块，多尺度特征
- 2015：ResNet - 残差连接，训练超深网络

**生成模型**：
- 2014：GAN - 对抗训练，生成逼真图像
- 2013：VAE - 变分推断，学习潜在表示

**自然语言处理**：
- 2013：Word2Vec - 词向量，捕获语义关系
- 2014：Seq2Seq - 编码器-解码器，端到端序列学习
- 2015：Attention - 注意力机制，动态关注

**关键技术**：
- ReLU激活函数 - 解决梯度消失
- Dropout - 防止过拟合
- Batch Normalization - 加速训练，稳定优化
- 残差连接 - 训练超深网络
- 注意力机制 - 捕获长距离依赖

**基础设施**：
- GPU训练成为标配
- 深度学习框架（Caffe、TensorFlow、PyTorch）
- 大规模数据集（ImageNet等）
- 开源社区的繁荣

这一时期为后来的大模型时代奠定了技术基础和思想基础。注意力机制的提出，更是直接孕育了2017年Transformer的诞生，开启了新的篇章。

---

**下一章预告**：[第三章：大模型时代 - 通用智能的曙光（2017-2024）](./04-第三章-大模型时代2017-2024.md)

我们将见证Transformer如何改变整个AI领域，GPT系列如何从1.17亿参数成长到1750亿参数，BERT如何统治NLP任务，以及ChatGPT如何引爆全民AI热潮。这是AI走向通用智能的关键时期。
