# Transformer前置数学知识

## 引言

本文档是《Transformer架构原理深度解析》的前置数学基础部分。在深入学习Transformer之前,掌握这些数学基础将帮助你更好地理解Transformer的核心机制和公式推导。

---

## 一、线性代数基础

### 1.1 向量与矩阵

#### **向量(Vector)**
- 向量是一个数的有序数组,可以表示为列向量或行向量
- 例如:x ∈ ℝ³ = [x₁, x₂, x₃]ᵀ

#### **矩阵(Matrix)**
- 矩阵是一个二维数组,包含m行n列
- 例如:A ∈ ℝ^(m×n)表示一个m行n列的矩阵

#### **关键性质**
```
矩阵维度表示:ℝ^(m×n) 表示m行n列的实数矩阵
向量是特殊的矩阵:ℝⁿ = ℝ^(n×1)
```

### 1.2 矩阵乘法

#### **定义**
设A ∈ ℝ^(m×n),B ∈ ℝ^(n×p),则C = AB ∈ ℝ^(m×p)

其中:C[i,j] = Σₖ A[i,k] · B[k,j]

#### **重要规则**
- **维度匹配**:A的列数必须等于B的行数
- **不满足交换律**:AB ≠ BA(一般情况)
- **满足结合律**:(AB)C = A(BC)

#### **Transformer中的应用**
```
Q = XW^Q
其中 X ∈ ℝ^(n×d_model), W^Q ∈ ℝ^(d_model×d_k)
结果 Q ∈ ℝ^(n×d_k)
```

**直观理解:**
在Transformer中,输入序列X(n个词,每个词d_model维)通过权重矩阵W^Q线性变换,得到Query矩阵Q。这是一个标准的矩阵乘法操作。

### 1.3 矩阵转置

#### **定义**
矩阵A ∈ ℝ^(m×n)的转置记为Aᵀ ∈ ℝ^(n×m)

满足:(Aᵀ)[i,j] = A[j,i]

#### **性质**
- (Aᵀ)ᵀ = A
- (AB)ᵀ = BᵀAᵀ
- (A + B)ᵀ = Aᵀ + Bᵀ

#### **Transformer中的应用**
```
QKᵀ:计算Query和Key的相似度
如果 Q ∈ ℝ^(n×d_k), K ∈ ℝ^(n×d_k)
则 QKᵀ ∈ ℝ^(n×n)
```

**具体例子:**
```
假设序列长度n=3,特征维度d_k=2

Q = [[1, 2],    K = [[3, 4],
     [3, 4],         [5, 6],
     [5, 6]]         [7, 8]]

Kᵀ = [[3, 5, 7],
      [4, 6, 8]]

QKᵀ = [[11, 17, 23],
       [25, 39, 53],
       [39, 61, 83]]

这个3×3矩阵表示每对Query-Key的相似度
```

### 1.4 点积(Dot Product)

#### **向量点积**
x · y = Σᵢ xᵢyᵢ = xᵀy

#### **几何意义**
x · y = ||x|| ||y|| cos(θ)

- 点积衡量两个向量的相似度
- 点积越大,两个向量越相似(方向越接近)
- 点积为0,两个向量正交(垂直)

#### **Transformer中的应用**
```
注意力分数 = Query · Key
表示Query和Key的相关性
```

**实际例子:**
```
Query = [1, 2, 3]  (查询"动物"的语义向量)
Key1  = [1, 2, 2]  (单词"猫"的语义向量)
Key2  = [0, 1, 0]  (单词"街道"的语义向量)

Query · Key1 = 1×1 + 2×2 + 3×2 = 11  (高相关性)
Query · Key2 = 1×0 + 2×1 + 3×0 = 2   (低相关性)

这就是Self-Attention的核心:通过点积计算相关性
```

### 1.5 矩阵的范数

#### **向量L2范数(欧几里得范数)**
||x||₂ = √(Σᵢ xᵢ²)

#### **作用**
- 衡量向量的"长度"
- 归一化:x_norm = x / ||x||₂

#### **Transformer中的应用**
```
1. Layer Normalization使用L2范数进行归一化
2. 梯度裁剪:限制梯度的L2范数不超过阈值
```

---

## 二、概率论与统计基础

### 2.1 概率分布

#### **离散概率分布**
对于离散随机变量X,满足:
- P(X = xᵢ) ≥ 0
- Σᵢ P(X = xᵢ) = 1

#### **期望(Expected Value)**
E[X] = Σᵢ xᵢ · P(X = xᵢ)

**直观理解**:期望是随机变量的"平均值"

#### **方差(Variance)**
Var(X) = E[(X - E[X])²] = E[X²] - (E[X])²

**直观理解**:方差衡量随机变量的"波动程度"

#### **标准差(Standard Deviation)**
σ = √Var(X)

#### **Transformer中的应用**
```
Layer Normalization计算均值和方差:
μ = E[x] = (1/d) Σᵢ xᵢ
σ² = Var(x) = (1/d) Σᵢ (xᵢ - μ)²

然后进行标准化:
x_normalized = (x - μ) / √(σ² + ε)
```

**具体例子:**
```
假设某层输出 x = [1, 2, 3, 4, 5]
均值 μ = (1+2+3+4+5)/5 = 3
方差 σ² = [(1-3)² + (2-3)² + (3-3)² + (4-3)² + (5-3)²]/5 = 2
标准差 σ = √2 ≈ 1.41

归一化后:
x_norm = [(1-3)/1.41, (2-3)/1.41, ..., (5-3)/1.41]
       ≈ [-1.41, -0.71, 0, 0.71, 1.41]
```

### 2.2 条件概率

#### **定义**
P(A|B) = P(A ∩ B) / P(B)

#### **链式法则**
P(x₁, x₂, ..., xₙ) = P(x₁) · P(x₂|x₁) · P(x₃|x₁,x₂) · ... · P(xₙ|x₁,...,xₙ₋₁)

#### **Transformer中的应用**
```
自回归语言模型(如GPT):
P(sentence) = P(w₁) · P(w₂|w₁) · P(w₃|w₁,w₂) · ...
```

**实际例子:**
```
句子:"我 爱 编程"

P(我 爱 编程) = P(我) × P(爱|我) × P(编程|我 爱)

GPT模型逐个预测下一个词:
1. 输入:[]      → 预测:P(我)
2. 输入:[我]    → 预测:P(爱|我)
3. 输入:[我 爱] → 预测:P(编程|我 爱)
```

---

## 三、微积分基础

### 3.1 导数与梯度

#### **一元函数导数**
f'(x) = lim[h→0] (f(x+h) - f(x)) / h

**直观理解**:导数是函数在某点的瞬时变化率(切线斜率)

#### **偏导数**
对多元函数f(x₁, x₂, ..., xₙ),关于xᵢ的偏导数:
∂f/∂xᵢ = lim[h→0] (f(x₁,...,xᵢ+h,...,xₙ) - f(x₁,...,xᵢ,...,xₙ)) / h

**直观理解**:固定其他变量,只让xᵢ变化时的变化率

#### **梯度(Gradient)**
∇f = [∂f/∂x₁, ∂f/∂x₂, ..., ∂f/∂xₙ]ᵀ

**直观理解**:梯度指向函数增长最快的方向

#### **Transformer中的应用**
```
反向传播计算梯度:
∂Loss/∂W = ∇_W Loss

参数更新(梯度下降):
W ← W - α · ∇_W Loss

其中α是学习率
```

**具体例子:**
```
假设损失函数 Loss = (y_pred - y_true)²
当前预测 y_pred = 5, 真实值 y_true = 3

Loss = (5 - 3)² = 4

∂Loss/∂y_pred = 2(y_pred - y_true) = 2(5-3) = 4

梯度为正,说明增加y_pred会增加Loss
所以应该减小y_pred → y_pred ← y_pred - α×4
```

### 3.2 链式法则(Chain Rule)

#### **一元情况**
如果 y = f(u), u = g(x),则:
dy/dx = (dy/du) · (du/dx)

#### **多元情况**
如果 z = f(x, y), x = g(t), y = h(t),则:
dz/dt = (∂z/∂x) · (dx/dt) + (∂z/∂y) · (dy/dt)

#### **Transformer中的应用**
```
反向传播的核心:
∂Loss/∂W = (∂Loss/∂y) · (∂y/∂W)

多层网络:
∂Loss/∂W₁ = (∂Loss/∂y₃) · (∂y₃/∂y₂) · (∂y₂/∂y₁) · (∂y₁/∂W₁)
```

**具体例子:**
```
前向传播:
x → W₁ → h₁ → W₂ → h₂ → W₃ → y → Loss

反向传播(链式法则):
∂Loss/∂W₁ = ∂Loss/∂y × ∂y/∂h₂ × ∂h₂/∂W₂ × ∂W₂/∂h₁ × ∂h₁/∂W₁

梯度从输出层逐层传回输入层
```

### 3.3 常用函数的导数

#### **基础导数**
- (xⁿ)' = n · xⁿ⁻¹
- (eˣ)' = eˣ
- (ln x)' = 1/x
- (sin x)' = cos x
- (cos x)' = -sin x

#### **Transformer中常用激活函数的导数**

**ReLU(x) = max(0, x)**
```
ReLU'(x) = {1  if x > 0
           {0  if x ≤ 0
```

**GELU(x) ≈ x · Φ(x)**  (Φ是标准正态分布的CDF)
```
GELU'(x) ≈ Φ(x) + x · φ(x)
其中φ(x)是标准正态分布的PDF
```

**Sigmoid(x) = 1/(1+e⁻ˣ)**
```
Sigmoid'(x) = Sigmoid(x) · (1 - Sigmoid(x))
```

---

## 四、Softmax函数详解

Softmax是Transformer中最重要的函数之一,用于将任意实数向量转换为概率分布。

### 4.1 定义与性质

#### **Softmax函数**
对于向量z = [z₁, z₂, ..., zₙ],Softmax定义为:

```
softmax(zᵢ) = exp(zᵢ) / Σⱼ exp(zⱼ)
```

#### **性质**
1. **输出范围**:(0, 1)
2. **概率分布**:Σᵢ softmax(zᵢ) = 1
3. **保持序关系**:zᵢ > zⱼ ⇒ softmax(zᵢ) > softmax(zⱼ)
4. **平滑的最大值**:softmax强调最大值,但保留其他值的信息

#### **具体例子**
```
输入 z = [1.0, 2.0, 3.0]

步骤1:计算指数
exp(z) = [e¹, e², e³] = [2.72, 7.39, 20.09]

步骤2:求和
Σ exp(zⱼ) = 2.72 + 7.39 + 20.09 = 30.20

步骤3:归一化
softmax(z) = [2.72/30.20, 7.39/30.20, 20.09/30.20]
           = [0.09, 0.24, 0.67]

注意:输出和为1,且最大的z₃对应最大的概率0.67
```

### 4.2 温度参数

Softmax可以引入温度参数τ来控制分布的平滑程度:

```
softmax_τ(zᵢ) = exp(zᵢ/τ) / Σⱼ exp(zⱼ/τ)
```

#### **温度的影响**
- **τ → 0**:接近one-hot(硬选择),最大值的概率接近1
- **τ = 1**:标准softmax
- **τ → ∞**:接近均匀分布,所有概率接近1/n

**具体例子:**
```
输入 z = [1.0, 2.0, 3.0]

τ = 0.5 (低温,尖锐分布):
softmax([2.0, 4.0, 6.0]) ≈ [0.002, 0.047, 0.951]

τ = 1.0 (标准):
softmax([1.0, 2.0, 3.0]) ≈ [0.09, 0.24, 0.67]

τ = 2.0 (高温,平滑分布):
softmax([0.5, 1.0, 1.5]) ≈ [0.19, 0.29, 0.52]
```

### 4.3 Softmax的梯度

#### **梯度公式**
```
∂softmax(zᵢ)/∂zⱼ = {
  softmax(zᵢ) · (1 - softmax(zᵢ))  if i = j
  -softmax(zᵢ) · softmax(zⱼ)        if i ≠ j
}
```

#### **雅可比矩阵**
Softmax的雅可比矩阵为:
```
J[i,j] = ∂softmax(zᵢ)/∂zⱼ
       = diag(softmax(z)) - softmax(z) ⊗ softmax(z)ᵀ
```

### 4.4 数值稳定性

#### **问题**
直接计算exp(zᵢ)可能导致数值溢出:
- 如果z = [1000, 1001, 1002],则exp(1002)会溢出

#### **解决方案**
```
softmax(zᵢ) = exp(zᵢ - max(z)) / Σⱼ exp(zⱼ - max(z))
```

**为什么有效?**
```
softmax(zᵢ) = exp(zᵢ) / Σⱼ exp(zⱼ)
            = exp(zᵢ - c) / Σⱼ exp(zⱼ - c)  (对任意常数c成立)

选择 c = max(z),确保所有 exp(zⱼ - c) ≤ 1,避免溢出
```

**具体例子:**
```
输入 z = [1000, 1001, 1002]

直接计算:exp(1002) = ∞ (溢出!)

数值稳定版本:
max(z) = 1002
z' = z - 1002 = [-2, -1, 0]
exp(z') = [0.135, 0.368, 1.0]
softmax(z) = [0.09, 0.24, 0.67]
```

### 4.5 Transformer中的应用

#### **注意力权重计算**
```
Attention(Q, K, V) = softmax(QKᵀ / √d_k) V

步骤:
1. 计算注意力分数:scores = QKᵀ / √d_k
2. 应用softmax:weights = softmax(scores)
3. 加权求和:output = weights × V
```

**为什么使用Softmax?**
1. **归一化**:注意力权重和为1,符合"分配注意力"的直觉
2. **可微**:支持反向传播
3. **平滑**:相比hard attention,梯度更稳定

---

## 五、信息论基础

### 5.1 熵(Entropy)

#### **定义**
对于离散分布P,熵衡量不确定性:
```
H(P) = -Σᵢ P(xᵢ) log P(xᵢ)
```

#### **直观理解**
- 熵越大,不确定性越高
- 均匀分布熵最大:H = log n
- 确定性分布(one-hot)熵最小:H = 0

#### **具体例子**
```
例1:均匀分布 P = [0.25, 0.25, 0.25, 0.25]
H = -4 × (0.25 × log 0.25) = -4 × 0.25 × (-1.39) = 1.39 bits

例2:非均匀分布 P = [0.7, 0.1, 0.1, 0.1]
H = -(0.7 log 0.7 + 3 × 0.1 log 0.1)
  = -(0.7×(-0.36) + 0.3×(-2.30))
  = 0.25 + 0.69 = 0.94 bits

均匀分布的不确定性更高(1.39 > 0.94)
```

### 5.2 交叉熵(Cross Entropy)

#### **定义**
衡量两个概率分布P和Q的差异:
```
H(P, Q) = -Σᵢ P(xᵢ) log Q(xᵢ)
```

- P:真实分布
- Q:预测分布

#### **Transformer中的应用**
```
分类损失函数:
Loss = -Σᵢ y_true(i) log y_pred(i)

其中:
- y_true是真实分布(通常是one-hot)
- y_pred是模型预测的概率分布(softmax输出)
```

**具体例子:**
```
3分类问题:
y_true = [0, 1, 0]  (真实类别是第2类)
y_pred = [0.1, 0.7, 0.2]  (模型预测)

Cross Entropy Loss:
Loss = -(0×log(0.1) + 1×log(0.7) + 0×log(0.2))
     = -log(0.7)
     = 0.36

如果预测更准确:y_pred = [0.05, 0.9, 0.05]
Loss = -log(0.9) = 0.11 (更小,更好!)
```

### 5.3 KL散度(Kullback-Leibler Divergence)

#### **定义**
```
KL(P || Q) = Σᵢ P(xᵢ) log(P(xᵢ) / Q(xᵢ))
           = H(P, Q) - H(P)
```

#### **性质**
1. **非负性**:KL(P || Q) ≥ 0
2. **当且仅当P=Q时为0**
3. **不对称**:KL(P || Q) ≠ KL(Q || P)

#### **与交叉熵的关系**
```
H(P, Q) = H(P) + KL(P || Q)

在训练中:
- H(P)是常数(真实分布固定)
- 最小化H(P, Q) ⟺ 最小化KL(P || Q)
```

---

## 六、优化算法基础

### 6.1 梯度下降算法家族

#### **批量梯度下降(Batch GD)**
```
W ← W - α · ∇_W Loss_all_data
```

- 优点:收敛稳定
- 缺点:计算慢,内存占用大

#### **随机梯度下降(Stochastic GD)**
```
W ← W - α · ∇_W Loss_single_sample
```

- 优点:更新快
- 缺点:波动大,可能不收敛

#### **小批量梯度下降(Mini-batch GD)**
```
W ← W - α · (1/batch_size) Σᵢ ∇_W Loss_i
```

- 平衡了速度和稳定性
- Transformer训练的标准选择

### 6.2 动量优化(Momentum)

#### **动机**
梯度下降可能在峡谷区域震荡,收敛慢

#### **算法**
```
v ← β · v + (1 - β) · g    # g是当前梯度
W ← W - α · v
```

- β通常取0.9
- v是梯度的"指数移动平均"
- 减少震荡,加速收敛

#### **直观理解**
想象一个球滚下山坡:
- 普通梯度下降:球每步只看当前斜率
- 动量优化:球积累了"速度",有惯性

### 6.3 Adam优化器

Adam(Adaptive Moment Estimation)是Transformer训练的标准优化器。

#### **算法**
```
# 初始化
m ← 0  (一阶矩估计,梯度均值)
v ← 0  (二阶矩估计,梯度平方均值)

# 每次迭代
g ← ∇_W Loss                      # 计算梯度
m ← β₁ · m + (1 - β₁) · g        # 更新一阶矩
v ← β₂ · v + (1 - β₂) · g²       # 更新二阶矩
m̂ ← m / (1 - β₁ᵗ)                # 偏差修正
v̂ ← v / (1 - β₂ᵗ)
W ← W - α · m̂ / (√v̂ + ε)        # 参数更新
```

#### **参数设置**
Transformer标准参数:
- β₁ = 0.9
- β₂ = 0.98 (比通常的0.999更小,更快适应)
- ε = 10⁻⁹
- α:动态调整(见学习率调度)

#### **为什么有效?**
1. **自适应学习率**:每个参数有独立的学习率
2. **动量**:一阶矩提供平滑
3. **归一化**:二阶矩防止学习率过大
4. **偏差修正**:早期训练时m和v偏向0,修正这个偏差

**具体例子:**
```
假设梯度序列:g₁=10, g₂=10, g₃=1, g₄=1

普通SGD (α=0.1):
W₁ = W₀ - 0.1×10 = W₀ - 1
W₂ = W₁ - 0.1×10 = W₀ - 2
W₃ = W₂ - 0.1×1  = W₀ - 2.1
W₄ = W₃ - 0.1×1  = W₀ - 2.2

Adam (简化):
前两步:大梯度 → 快速更新
后两步:小梯度 → 自动减小学习率,精细调整
```

### 6.4 学习率调度

#### **Transformer的学习率策略**
```
lr = d_model^(-0.5) · min(step^(-0.5), step · warmup_steps^(-1.5))
```

**分段理解:**
1. **Warmup阶段**(step < warmup_steps):
   ```
   lr = d_model^(-0.5) · step · warmup_steps^(-1.5)
   ```
   学习率线性增长

2. **Decay阶段**(step ≥ warmup_steps):
   ```
   lr = d_model^(-0.5) · step^(-0.5)
   ```
   学习率按平方根倒数衰减

#### **为什么这样设计?**
- **Warmup**:初始参数随机,大学习率容易发散
- **Decay**:后期需要精细调整,降低学习率

**具体例子:**
```
假设 d_model=512, warmup_steps=4000

步骤    学习率(相对值)
1000    0.000354 (warmup阶段,线性增长)
2000    0.000707
4000    0.001414 (峰值)
8000    0.001000 (decay阶段,平方根衰减)
16000   0.000707
```

---

## 七、数值稳定性技巧

### 7.1 归一化(Normalization)

#### **为什么需要归一化?**
1. **防止梯度爆炸/消失**:激活值在合理范围
2. **加速收敛**:输入分布稳定
3. **提高数值稳定性**:避免浮点溢出

#### **Layer Normalization公式**
```
LN(x) = γ · (x - μ) / √(σ² + ε) + β

其中:
μ = (1/d) Σᵢ xᵢ           # 均值
σ² = (1/d) Σᵢ (xᵢ - μ)²   # 方差
γ, β 是可学习参数(仿射变换)
ε = 10⁻⁵ (防止除零)
```

#### **Batch Norm vs Layer Norm**

**Batch Normalization:**
- 对batch维度归一化
- 适合CNN,图像任务
- 问题:序列长度不同时效果差

**Layer Normalization:**
- 对特征维度归一化
- 适合RNN、Transformer
- 每个样本独立计算

**具体对比:**
```
假设输入 X ∈ ℝ^(batch=2, seq_len=3, d_model=4)

Batch Norm:对每个位置、每个特征,跨batch归一化
→ 对X[:, i, j]进行归一化(2个数)

Layer Norm:对每个样本、每个位置,跨特征归一化
→ 对X[b, i, :]进行归一化(4个数)
```

### 7.2 梯度裁剪(Gradient Clipping)

#### **问题**
深度网络可能出现梯度爆炸,导致参数更新过大,训练不稳定。

#### **解决方案**
```
if ||∇W|| > threshold:
    ∇W ← (threshold / ||∇W||) · ∇W
```

#### **Transformer中的应用**
通常设置threshold=1.0或5.0

**具体例子:**
```
假设梯度 g = [3, 4], threshold=1.0

||g|| = √(3² + 4²) = 5 > 1.0

裁剪后:
g_clipped = (1.0/5.0) · [3, 4] = [0.6, 0.8]
```

### 7.3 残差连接的数学意义

#### **标准残差**
```
y = F(x) + x
```

#### **梯度流动分析**
```
∂Loss/∂x = ∂Loss/∂y · ∂y/∂x
         = ∂Loss/∂y · (∂F/∂x + I)
```

- I是单位矩阵
- 即使∂F/∂x很小(梯度消失),∂y/∂x至少为1
- 梯度可以直接通过残差连接流回

#### **为什么有效?**
1. **缓解梯度消失**:保证梯度至少能传回
2. **加速收敛**:提供直接路径
3. **支持深层网络**:100+层仍可训练

**对比实验(概念):**
```
无残差连接:
Layer 100 → ... → Layer 1
梯度需要经过100层,容易消失

有残差连接:
Layer 100 ⇉ Layer 1 (直接路径)
        ⤷ 经过各层 (学习路径)
```

### 7.4 初始化策略

#### **Xavier/Glorot初始化**
```
W ~ U(-√(6/(n_in + n_out)), √(6/(n_in + n_out)))
```

- 适合tanh、sigmoid激活函数
- 保持方差稳定

#### **He初始化**
```
W ~ N(0, √(2/n_in))
```

- 适合ReLU激活函数
- Transformer中常用

---

## 八、重要不等式与定理

### 8.1 Jensen不等式

#### **定理**
对于凸函数f和概率分布P:
```
f(E[X]) ≤ E[f(X)]
```

对于凹函数,不等号反向。

#### **应用**
用于推导KL散度的非负性:
```
KL(P || Q) = Σᵢ P(xᵢ) log(P(xᵢ)/Q(xᵢ))
           = E_P[log(P/Q)]
           ≥ log(E_P[P/Q])  (Jensen不等式,log是凹函数)
           = log(Σᵢ Q(xᵢ))
           = log(1) = 0
```

### 8.2 三角不等式

#### **向量形式**
```
||x + y|| ≤ ||x|| + ||y||
```

#### **应用**
分析残差连接的稳定性:
```
||F(x) + x|| ≤ ||F(x)|| + ||x||
```

即使F(x)很大,通过残差连接的输出仍有界。

---

## 九、数学符号表

| 符号 | 含义 | 示例 |
|------|------|------|
| ∈ | 属于 | x ∈ ℝⁿ |
| ℝ | 实数集 | ℝ³表示三维实数空间 |
| ℝ^(m×n) | m×n实数矩阵 | A ∈ ℝ^(512×512) |
| Σ | 求和 | Σᵢ xᵢ = x₁ + x₂ + ... |
| ∏ | 连乘 | ∏ᵢ xᵢ = x₁ · x₂ · ... |
| ∇ | 梯度 | ∇f = [∂f/∂x₁, ..., ∂f/∂xₙ] |
| ∂ | 偏导数 | ∂f/∂x |
| ⊙ | 逐元素乘法(Hadamard积) | (a ⊙ b)ᵢ = aᵢ · bᵢ |
| ⊕ | 逐元素加法 | (a ⊕ b)ᵢ = aᵢ + bᵢ |
| ⊗ | 外积 | a ⊗ b = abᵀ |
| || · || | 范数 | ||x||₂ = √(Σᵢ xᵢ²) |
| || · ||₁ | L1范数 | ||x||₁ = Σᵢ |xᵢ| |
| || · ||₂ | L2范数(欧几里得范数) | ||x||₂ = √(Σᵢ xᵢ²) |
| || · ||∞ | 无穷范数 | ||x||∞ = max|xᵢ| |
| ᵀ | 转置 | Aᵀ |
| ⁻¹ | 逆矩阵 | A⁻¹ |
| E[·] | 期望 | E[X] = Σᵢ xᵢP(xᵢ) |
| Var[·] | 方差 | Var[X] = E[(X-E[X])²] |
| P(·) | 概率 | P(X=x) |
| log | 自然对数 | log(e) = 1 |
| ln | 自然对数(同log) | ln(e) = 1 |
| exp | 指数函数 | exp(x) = eˣ |
| argmax | 最大值的索引 | argmax_i f(xᵢ) |
| argmin | 最小值的索引 | argmin_i f(xᵢ) |
| := | 定义为 | f(x) := x² + 1 |
| ≈ | 约等于 | π ≈ 3.14 |
| ∝ | 正比于 | P(x) ∝ exp(-x²) |
| ⟹ | 推出 | A ⟹ B |
| ⟺ | 等价 | A ⟺ B |
| ∀ | 对所有 | ∀x ∈ ℝ |
| ∃ | 存在 | ∃x such that f(x)=0 |

---

## 十、数学知识自测

### 10.1 线性代数测试

**问题1:**
给定 Q ∈ ℝ^(10×64), K ∈ ℝ^(10×64),计算QKᵀ的形状?

<details>
<summary>答案</summary>
QKᵀ ∈ ℝ^(10×10)

解释:Q(10×64) × Kᵀ(64×10) = (10×10)
</details>

**问题2:**
向量x=[1,1,1,1],计算其L2范数?

<details>
<summary>答案</summary>
||x||₂ = √(1²+1²+1²+1²) = √4 = 2
</details>

### 10.2 概率论测试

**问题3:**
分布P=[0.5, 0.3, 0.2],计算熵H(P)?

<details>
<summary>答案</summary>
H(P) = -(0.5 log 0.5 + 0.3 log 0.3 + 0.2 log 0.2)
     ≈ -(0.5×(-0.69) + 0.3×(-1.20) + 0.2×(-1.61))
     ≈ 0.345 + 0.360 + 0.322
     ≈ 1.03 nats (或1.49 bits)
</details>

### 10.3 Softmax测试

**问题4:**
z=[0,1,2],计算softmax(z)?

<details>
<summary>答案</summary>
exp(z) = [1, 2.72, 7.39]
Σ = 11.11
softmax(z) = [1/11.11, 2.72/11.11, 7.39/11.11]
           ≈ [0.09, 0.24, 0.67]
</details>

---

## 总结:为什么这些数学知识重要?

### 对应关系表

| 数学概念 | Transformer中的应用 | 重要性 |
|----------|---------------------|--------|
| 矩阵乘法 | Q=XW^Q, QKᵀ | ⭐⭐⭐⭐⭐ |
| 点积 | 注意力分数计算 | ⭐⭐⭐⭐⭐ |
| Softmax | 注意力权重归一化 | ⭐⭐⭐⭐⭐ |
| LayerNorm | 稳定训练 | ⭐⭐⭐⭐⭐ |
| 梯度/链式法则 | 反向传播 | ⭐⭐⭐⭐⭐ |
| 交叉熵 | 训练损失函数 | ⭐⭐⭐⭐⭐ |
| Adam优化器 | 参数更新 | ⭐⭐⭐⭐⭐ |
| 残差连接 | 深层网络训练 | ⭐⭐⭐⭐ |
| 梯度裁剪 | 防止梯度爆炸 | ⭐⭐⭐⭐ |
| 条件概率 | 语言模型概率建模 | ⭐⭐⭐ |

---

## 下一步学习

掌握了这些数学基础后,你已经具备理解Transformer的必要知识。建议:

1. **立即开始**:阅读《Transformer架构原理深度解析》主文档
2. **对照学习**:遇到公式时回查本文档
3. **动手实践**:用Python实现这些数学函数
4. **可视化**:用Matplotlib绘制Softmax、梯度等函数

**记住**:数学是工具,不是目的。重点是理解Transformer的设计思想,数学帮助我们精确表达和实现这些思想。

---

**推荐资源:**
1. 线性代数:MIT 18.06 Linear Algebra (Gilbert Strang)
2. 概率论:Probability and Statistics (Stanford CS109)
3. 深度学习数学:《Deep Learning》by Goodfellow (Chapter 2-4)
4. 交互式学习:3Blue1Brown的线性代数系列视频
