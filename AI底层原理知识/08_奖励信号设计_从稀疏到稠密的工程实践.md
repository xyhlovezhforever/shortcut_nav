# 奖励信号设计：从稀疏到稠密的工程实践

## 一、奖励函数的核心作用

### 1.1 什么是奖励函数?

在强化学习中，奖励函数是模型唯一的"指南针"：

```
传统监督学习：告诉模型"正确答案是什么"
强化学习：告诉模型"这个输出有多好"

数学形式：R: (状态, 动作) → 实数
         R(s, a) = 评分值
```

**案例对比**：

| 任务 | 监督学习标签 | 强化学习奖励 |
|------|-------------|-------------|
| 数学题 | 标准解题步骤 | 答案正确→+1，错误→0 |
| 代码生成 | 参考实现 | 通过测试用例数量 |
| 对话 | 优质回复样本 | 用户满意度评分 |

### 1.2 奖励设计的核心挑战

**问题1：稀疏性（Sparsity）**
```python
# 数学推理任务
问题: 证明费马大定理...
模型输出: [500个推理token]
奖励: 0（因为最终答案错误）

问题：模型不知道哪个步骤出错了
→ 学习信号极弱
```

**问题2：欺骗性（Reward Hacking）**
```python
# 天真的长度奖励
R = correctness - 0.01 * length

模型学习策略：
输出 = "答案是42"  # 极短但错误
R = 0 - 0.01 * 10 = -0.1

输出 = "根据题意分析...[正确推理]"
R = 1 - 0.01 * 500 = -4.0  # 奖励更低！

→ 模型学会偷懒
```

**问题3：尺度问题（Scaling）**
```python
# 多目标奖励
R = 100 * accuracy + 0.01 * fluency + 0.001 * efficiency

问题：accuracy占主导，其他项被忽略
→ 模型只优化准确性
```

---

## 二、JustRL的极简奖励设计

### 2.1 核心哲学：简单但正确

**JustRL奖励函数**：
```python
def compute_reward(model_output, ground_truth):
    """极简但有效的奖励"""
    # 提取模型答案
    model_answer = extract_final_answer(model_output)

    # 二值奖励
    if is_correct(model_answer, ground_truth):
        return 1.0
    else:
        return 0.0

# 就这么简单！没有任何附加项
```

**为什么这样可行？**

1. **清晰的目标信号**：
   - 模型明确知道"正确答案"是唯一目标
   - 没有混淆的多目标权衡

2. **避免人为偏见**：
   - 不惩罚长输出（模型自然学会简洁）
   - 不奖励中间步骤（避免奖励错误推理）

3. **可靠的验证**：
   - 数学答案验证准确（不像主观质量评分）
   - 减少噪声标注

### 2.2 答案提取的工程实践

**挑战**：模型输出格式不统一
```
输出1: "因此答案是 42"
输出2: "综上所述，x = 42"
输出3: "Final Answer: 42"
输出4: "42"
```

**鲁棒提取器**：
```python
import re

def extract_final_answer(text):
    """多策略答案提取"""

    # 策略1：查找明确的答案标记
    patterns = [
        r'(?:最终答案|答案|Final Answer)[：:]\s*(.+)',
        r'(?:因此|所以|故)[，,]?\s*(?:答案是|答案为)\s*(.+)',
        r'\\boxed\{(.+?)\}',  # LaTeX格式
    ]

    for pattern in patterns:
        match = re.search(pattern, text, re.IGNORECASE)
        if match:
            return match.group(1).strip()

    # 策略2：提取最后一个数字/表达式
    numbers = re.findall(r'-?\d+(?:\.\d+)?', text)
    if numbers:
        return numbers[-1]

    # 策略3：返回最后一句话
    sentences = text.split('。')
    return sentences[-1].strip()

# 验证函数
def is_correct(model_answer, ground_truth):
    """数值容忍比较"""
    try:
        # 转换为数值
        model_val = float(eval(model_answer))  # 支持"1/2"等
        truth_val = float(ground_truth)

        # 相对误差<1%
        return abs(model_val - truth_val) / (abs(truth_val) + 1e-8) < 0.01
    except:
        # 字符串精确匹配
        return model_answer.strip().lower() == ground_truth.strip().lower()
```

### 2.3 实验验证：简单奖励的有效性

**对比实验**：

| 奖励设计 | AIME准确率 | 平均长度 | 训练稳定性 |
|---------|-----------|---------|-----------|
| 二值（JustRL） | **58.6%** | 3500 | 平滑 |
| +长度惩罚 | 45.2% | 2800 | 震荡 |
| +步骤奖励 | 52.1% | 4200 | 中等 |
| +验证器集成 | 54.3% | 3600 | 中等 |

**关键发现**：
- ✅ 简单二值奖励效果最好
- ❌ 添加人工设计的辅助奖励反而降低性能

---

## 三、场景实战：不同任务的奖励设计

### 3.1 场景1：代码生成任务

**目标**：生成通过测试用例的代码

**奖励设计（初级）**：
```python
def code_reward_v1(generated_code, test_cases):
    """基于通过率的奖励"""
    passed = 0
    total = len(test_cases)

    for test in test_cases:
        try:
            # 执行代码
            exec_result = execute_code(generated_code, test["input"])

            # 检查输出
            if exec_result == test["expected_output"]:
                passed += 1
        except Exception:
            pass  # 运行错误计为失败

    # 通过率奖励
    return passed / total

# 问题：0.5和0.6的差距太小，学习信号弱
```

**奖励设计（改进）**：
```python
def code_reward_v2(generated_code, test_cases):
    """分层奖励"""
    passed = 0
    total = len(test_cases)

    for test in test_cases:
        try:
            exec_result = execute_code(generated_code, test["input"])

            if exec_result == test["expected_output"]:
                passed += 1
        except SyntaxError:
            return -0.5  # 语法错误严重惩罚
        except TimeoutError:
            return -0.2  # 超时轻度惩罚
        except:
            pass

    if passed == total:
        return 1.0  # 全部通过高奖励
    elif passed > 0:
        return 0.3 + 0.7 * (passed / total)  # 部分通过
    else:
        return 0.0  # 全部失败
```

**实验结果**：
- v1（线性奖励）：最终通过率 68%
- v2（分层奖励）：最终通过率 **78%**（提升15%）

### 3.2 场景2：长文本摘要

**目标**：生成简洁准确的摘要

**奖励设计（多维度）**：
```python
def summary_reward(summary, reference, original_text):
    """组合多个自动指标"""

    # 1. ROUGE-L（覆盖度）
    rouge_score = compute_rouge_l(summary, reference)

    # 2. 长度奖励（鼓励简洁）
    length_ratio = len(summary) / len(original_text)
    if 0.1 < length_ratio < 0.3:  # 10%-30%为理想
        length_reward = 1.0
    else:
        length_reward = 0.5

    # 3. 事实一致性（使用NLI模型）
    consistency_score = check_factual_consistency(summary, original_text)

    # 加权组合
    reward = (
        0.5 * rouge_score +
        0.2 * length_reward +
        0.3 * consistency_score
    )

    return reward

def check_factual_consistency(summary, source):
    """使用预训练NLI模型检查事实"""
    from transformers import pipeline

    nli_model = pipeline("text-classification", model="roberta-large-mnli")

    # 将摘要每句与原文对比
    summary_sents = split_sentences(summary)
    scores = []

    for sent in summary_sents:
        result = nli_model(f"{source} [SEP] {sent}")
        # entailment（蕴含）得分高→一致性好
        entail_prob = [r for r in result if r["label"] == "ENTAILMENT"][0]["score"]
        scores.append(entail_prob)

    return sum(scores) / len(scores)
```

**实验数据**：
| 奖励设计 | ROUGE-L | 事实准确率 | 用户满意度 |
|---------|---------|-----------|-----------|
| 仅ROUGE | 0.42 | 78% | 6.2/10 |
| ROUGE+长度 | 0.45 | 81% | 7.1/10 |
| **完整组合** | **0.48** | **89%** | **8.3/10** |

### 3.3 场景3：对话系统

**目标**：生成有帮助、无害、诚实的回复

**奖励设计（使用奖励模型）**：
```python
class RewardModel:
    """从人类偏好数据训练的奖励模型"""

    def __init__(self, model_path):
        from transformers import AutoModelForSequenceClassification
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)

    def compute_reward(self, prompt, response):
        """预测人类偏好分数"""
        input_text = f"{prompt} [SEP] {response}"
        inputs = self.tokenizer(input_text, return_tensors="pt")

        with torch.no_grad():
            outputs = self.model(**inputs)
            reward = outputs.logits[0].item()  # 回归输出

        return reward

# 训练奖励模型（准备阶段）
def train_reward_model(preference_data):
    """
    preference_data格式：
    [
        {
            "prompt": "如何学习Python?",
            "response_a": "推荐看官方文档",
            "response_b": "直接抄代码就行",
            "preference": "a"  # 人类标注a更好
        },
        ...
    ]
    """
    from transformers import AutoModelForSequenceClassification, Trainer

    model = AutoModelForSequenceClassification.from_pretrained(
        "bert-base-uncased",
        num_labels=1  # 回归任务
    )

    # 转换为训练数据
    train_data = []
    for item in preference_data:
        # 好回复得分1.0
        train_data.append({
            "text": f"{item['prompt']} [SEP] {item[f'response_{item['preference']}']}",
            "label": 1.0
        })

        # 差回复得分0.0
        other = "b" if item["preference"] == "a" else "a"
        train_data.append({
            "text": f"{item['prompt']} [SEP] {item[f'response_{other}']}",
            "label": 0.0
        })

    # 训练
    trainer = Trainer(model=model, train_dataset=train_data)
    trainer.train()

    return model
```

**实际部署注意事项**：
- 奖励模型需要定期更新（人类偏好会变化）
- 监控奖励分数分布（防止模型利用漏洞）
- 结合规则约束（如禁止输出有害内容）

---

## 四、奖励塑造技术（Reward Shaping）

### 4.1 什么是奖励塑造？

**定义**：在原始奖励基础上添加辅助信号，加速学习

**基本形式**：
```python
R_shaped(s, a, s') = R_original(s, a) + F(s, s')

其中 F(s, s') = γ * Φ(s') - Φ(s)  # 势函数差分
```

**理论保证**：如果满足上述形式，最优策略不变！

### 4.2 实战案例：多步推理任务

**原始奖励**：
```python
# 只有最终答案有反馈
R_final = 1 if correct else 0
```

**塑造奖励**：
```python
def shaped_reward(trajectory, final_answer):
    """添加中间步骤奖励"""
    R_total = 0

    # 最终奖励
    if is_correct(final_answer):
        R_total += 1.0

    # 中间步骤奖励（势函数）
    for i in range(len(trajectory) - 1):
        step_i = trajectory[i]
        step_i1 = trajectory[i + 1]

        # 势函数：到答案的"距离"估计
        Φ_i = estimate_progress(step_i)
        Φ_i1 = estimate_progress(step_i1)

        # 进步奖励
        R_total += 0.1 * (Φ_i1 - Φ_i)

    return R_total

def estimate_progress(step_text):
    """估计推理进度（0-1）"""
    # 简单启发式：提到关键词
    keywords = ["因此", "所以", "得出", "答案"]
    score = sum([1 for kw in keywords if kw in step_text])
    return min(score / len(keywords), 1.0)
```

**效果对比**：
- 无塑造：1000步达到40%准确率
- 有塑造：500步达到40%准确率（**加速2倍**）

**注意事项**：
- ⚠️ 势函数设计错误会改变最优策略
- ⚠️ JustRL实验发现简单任务不需要塑造

### 4.3 反例：有害的奖励塑造

**案例**：显式长度惩罚（JustRL论文实验）

```python
# 错误的塑造
R_bad = correctness - 0.001 * (length - target_length)^2

问题：
1. 破坏了原始目标（最优策略从"正确"变为"正确且短"）
2. 超参数敏感（0.001调成0.002结果大不同）
3. 阻碍探索（模型不敢尝试长推理）
```

**实验数据**（来自JustRL）：
| 配置 | AIME准确率 | 平均长度 | 策略熵 |
|------|-----------|---------|--------|
| 无惩罚 | 54.87% | 3500 | 1.3 |
| 弱惩罚(0.0001) | 52.3% | 3200 | 1.1 |
| 强惩罚(0.001) | **45.2%** | 2800 | **0.8** |

**教训**：不要为了"工程美感"而添加惩罚项

---

## 五、奖励函数调试技巧

### 5.1 检查清单

**1. 验证基本正确性**
```python
def test_reward_function():
    """单元测试奖励函数"""

    # 测试1：明显正确的答案
    assert compute_reward("42", "42") == 1.0

    # 测试2：明显错误的答案
    assert compute_reward("100", "42") == 0.0

    # 测试3：格式变化
    assert compute_reward("答案是42", "42") == 1.0

    # 测试4：数值容忍
    assert compute_reward("41.99", "42") == 1.0  # 1%误差内

    print("✅ 所有测试通过")
```

**2. 分析奖励分布**
```python
def analyze_reward_distribution(model, dataset):
    """统计奖励分布"""
    rewards = []

    for problem in dataset:
        output = model.generate(problem)
        reward = compute_reward(output, problem["answer"])
        rewards.append(reward)

    print(f"平均奖励: {np.mean(rewards):.3f}")
    print(f"标准差: {np.std(rewards):.3f}")
    print(f"最小值: {min(rewards):.3f}")
    print(f"最大值: {max(rewards):.3f}")

    # 警告信号
    if np.std(rewards) < 0.1:
        print("⚠️  奖励方差过小，学习信号弱")

    if np.mean(rewards) < 0.01:
        print("⚠️  平均奖励接近0，任务可能太难")
```

**3. 监控奖励欺骗**
```python
def detect_reward_hacking(model, train_rewards, val_rewards):
    """检测过拟合奖励函数"""

    # 训练集奖励涨，验证集不涨→可能在hack
    if train_rewards[-1] > train_rewards[0] * 1.5:
        if val_rewards[-1] < val_rewards[0] * 1.1:
            print("🚨 警告：可能存在奖励欺骗")
            print("   训练集奖励↑ 但验证集性能不变")

    # 人工抽查
    samples = model.generate(val_problems[:10])
    for i, (output, problem) in enumerate(zip(samples, val_problems)):
        reward = compute_reward(output, problem["answer"])
        print(f"\n样本{i}:")
        print(f"  输出: {output[:200]}")
        print(f"  奖励: {reward}")
        print(f"  真实质量: [需人工评估]")
```

### 5.2 常见Bug诊断

**Bug 1：浮点精度问题**
```python
# 错误
def is_correct_bad(model_answer, truth):
    return float(model_answer) == float(truth)  # 0.1+0.2≠0.3

# 正确
def is_correct_good(model_answer, truth):
    return abs(float(model_answer) - float(truth)) < 1e-6
```

**Bug 2：异常处理不当**
```python
# 错误
def compute_reward_bad(code, tests):
    passed = 0
    for test in tests:
        if execute(code, test) == test["expected"]:
            passed += 1
    return passed / len(tests)  # execute抛异常会中断

# 正确
def compute_reward_good(code, tests):
    passed = 0
    for test in tests:
        try:
            if execute(code, test) == test["expected"]:
                passed += 1
        except Exception as e:
            logging.warning(f"执行失败: {e}")
            pass  # 失败计为0分
    return passed / len(tests)
```

**Bug 3：奖励尺度不一致**
```python
# 错误：不同任务奖励范围差异大
task_a_reward = 0.0 - 1.0   # 数学题
task_b_reward = 0.0 - 100.0 # 代码（通过测试数量）

# 正确：归一化到同一尺度
def normalize_reward(raw_reward, task_type):
    if task_type == "math":
        return raw_reward  # 已在[0,1]
    elif task_type == "code":
        return raw_reward / 100.0  # 归一化
```

---

## 六、进阶话题：在线学习奖励函数

### 6.1 动机：人类标注成本高

**问题**：训练奖励模型需要大量人类偏好数据

**解决思路**：在RL训练过程中同步更新奖励模型

### 6.2 实现框架

```python
class OnlineRewardLearning:
    def __init__(self, policy_model, reward_model):
        self.policy = policy_model
        self.reward = reward_model

        self.human_feedback_buffer = []

    def train_step(self, prompts):
        # 1. 策略生成多个候选回复
        candidates = [
            self.policy.generate(p, num_return=4)
            for p in prompts
        ]

        # 2. 奖励模型打分
        rewards = [
            [self.reward.score(p, c) for c in cands]
            for p, cands in zip(prompts, candidates)
        ]

        # 3. 用最高奖励的样本更新策略
        best_candidates = [
            cands[np.argmax(rews)]
            for cands, rews in zip(candidates, rewards)
        ]
        self.update_policy(prompts, best_candidates, rewards)

        # 4. 随机抽样人类标注
        if random.random() < 0.01:  # 1%样本
            self.request_human_feedback(prompts[0], candidates[0])

    def request_human_feedback(self, prompt, candidates):
        """请求人类标注偏好"""
        # 实际部署中连接标注平台
        human_ranking = get_human_ranking(prompt, candidates)

        # 用新数据微调奖励模型
        self.human_feedback_buffer.append({
            "prompt": prompt,
            "ranking": human_ranking
        })

        if len(self.human_feedback_buffer) >= 100:
            self.finetune_reward_model()
            self.human_feedback_buffer = []

    def finetune_reward_model(self):
        """用人类反馈更新奖励模型"""
        # 转换为成对比较数据
        pairs = []
        for item in self.human_feedback_buffer:
            ranking = item["ranking"]  # [cand_2, cand_0, cand_1, cand_3]

            # 最好 vs 最差
            pairs.append({
                "prompt": item["prompt"],
                "chosen": ranking[0],
                "rejected": ranking[-1]
            })

        # 用对比学习微调
        self.reward.train_on_pairs(pairs)
```

### 6.3 实际效果

**案例**：对话系统持续改进
- 初始：基于1万条人类标注训练奖励模型
- 在线学习：每天额外标注100条新样本
- 结果：30天后准确率从82%提升到89%

---

## 七、总结与最佳实践

### 核心原则

1. **从简单开始**：先用最简单的奖励（如二值），验证有效后再考虑复杂化
2. **保证正确性**：奖励函数bug比模型bug更隐蔽，务必充分测试
3. **监控分布**：实时跟踪奖励的均值/方差/分布，及早发现异常
4. **避免过度工程**：不要为了"理论美感"添加不必要的惩罚项

### 推荐流程

```
第1周：设计基础奖励
  ├─ 明确任务目标（准确性、质量、安全性等）
  ├─ 实现最简单的奖励函数
  ├─ 编写单元测试验证正确性
  └─ 在小数据集上测试分布

第2周：验证有效性
  ├─ 用奖励训练小模型（如1.5B）
  ├─ 监控奖励曲线是否上升
  ├─ 人工检查高奖励样本质量
  └─ 确认无奖励欺骗

第3周：扩展优化
  ├─ 如需要，添加辅助奖励项
  ├─ 调整权重系数
  ├─ 在大模型上验证
  └─ 部署在线学习机制
```

### 工具推荐

**奖励模型训练**：
- [trlx](https://github.com/CarperAI/trlx)：支持RLHF全流程
- [DeepSpeed-Chat](https://github.com/microsoft/DeepSpeedExamples/tree/master/applications/DeepSpeed-Chat)：微软的RLHF框架

**评估指标**：
- 代码：执行通过率、单元测试覆盖
- 文本：ROUGE、BERTScore、人类评分
- 数学：符号等价检查（SymPy）

### 下一步学习

- [模型优化反模式：哪些技巧可能有害](09_LLM优化反模式_复杂技巧的负面案例.md)
- [大规模RL训练的工程实践](10_分布式RL训练_从单卡到千卡集群.md)
