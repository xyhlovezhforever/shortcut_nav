# 强化学习:马尔可夫决策过程与价值迭代

## 1. 引言

强化学习(Reinforcement Learning, RL)是机器学习的第三大范式,不同于监督学习和无监督学习,RL 通过**与环境交互**学习最优策略。本教程深入探讨 RL 的数学基础——**马尔可夫决策过程(MDP)**及求解方法。

---

## 2. 强化学习的基本概念

### 2.1 核心要素

```
Agent(智能体) ⇄ Environment(环境)
   ↓                 ↓
Action(动作)     State(状态)
   ↑                 ↓
   └─── Reward(奖励) ←┘
```

**交互流程:**
1. Agent 在状态 s_t 下执行动作 a_t
2. Environment 转移到新状态 s_(t+1),返回奖励 r_t
3. Agent 根据经验调整策略
4. 重复直到达到终止状态

### 2.2 与监督学习的对比

| 特性 | 监督学习 | 强化学习 |
|------|----------|----------|
| 反馈 | 每个样本有标签 | 延迟的奖励信号 |
| 数据分布 | i.i.d. | 序列相关 |
| 目标 | 拟合函数 | 最大化累积奖励 |
| 示例 | 图像分类 | 游戏AI, 机器人控制 |

---

## 3. 马尔可夫决策过程(MDP)

### 3.1 数学定义

MDP 是一个五元组:
```
M = (S, A, P, R, γ)
```

**组件:**
- **S**: 状态空间(有限或无限)
- **A**: 动作空间
- **P**: 状态转移概率 P(s'|s, a)
- **R**: 奖励函数 R(s, a, s')
- **γ**: 折扣因子 γ ∈ [0, 1)

### 3.2 马尔可夫性质

**核心假设:**
```
P(s_(t+1) | s_t, a_t, s_(t-1), a_(t-1), ..., s_0, a_0)
= P(s_(t+1) | s_t, a_t)
```

**含义:** 未来只依赖当前状态,与历史无关(给定当前)。

**为什么合理?**
- 当前状态已包含所有相关信息
- 简化模型,使问题可解

**反例:** 部分可观测环境(POMDP)

---

### 3.3 策略(Policy)

**定义:** 从状态到动作的映射

**(1) 确定性策略**
```
π: S → A
a = π(s)
```

**(2) 随机性策略**
```
π: S × A → [0, 1]
π(a|s) = P(a|s)
```

**为什么需要随机策略?**
- 探索未知区域
- 某些情况下最优策略是随机的(如石头剪刀布)

---

### 3.4 回报(Return)

**累积奖励:**
```
G_t = r_t + γ·r_(t+1) + γ²·r_(t+2) + ... = Σ γ^k · r_(t+k)
                                           k=0
```

**折扣因子 γ 的作用:**
1. **数学:** 保证无限序列收敛
   ```
   G_t ≤ Σ γ^k · R_max = R_max / (1-γ)
   ```

2. **实际意义:**
   - γ→1: 远见(long-term)
   - γ→0: 短视(immediate reward)
   - γ=0.99 常用于游戏

3. **不确定性:** 未来不可靠,应打折

---

## 4. 价值函数

### 4.1 状态价值函数(State-Value Function)

**定义:** 从状态 s 开始,遵循策略 π 的期望回报
```
V^π(s) = E_π[G_t | s_t = s]
       = E_π[Σ γ^k · r_(t+k) | s_t = s]
```

**直观理解:** "这个状态有多好?"

### 4.2 动作价值函数(Action-Value Function)

**定义:** 在状态 s 执行动作 a,然后遵循 π 的期望回报
```
Q^π(s, a) = E_π[G_t | s_t = s, a_t = a]
```

**直观理解:** "在这个状态下,这个动作有多好?"

### 4.3 价值函数之间的关系

**(1) V 与 Q 的关系**
```
V^π(s) = Σ π(a|s) · Q^π(s, a)
         a
```

**(2) Q 与 V 的关系**
```
Q^π(s, a) = Σ P(s'|s,a) · [R(s,a,s') + γ·V^π(s')]
            s'
```

---

## 5. 贝尔曼方程(Bellman Equation)

### 5.1 贝尔曼期望方程

**对 V^π:**
```
V^π(s) = Σ π(a|s) · Σ P(s'|s,a) · [R(s,a,s') + γ·V^π(s')]
         a          s'
```

**推导:**
```
V^π(s) = E_π[G_t | s_t = s]
       = E_π[r_t + γ·G_(t+1) | s_t = s]
       = E_π[r_t | s_t = s] + γ·E_π[G_(t+1) | s_t = s]
       = Σ π(a|s) · Σ P(s'|s,a) · R(s,a,s')
         a          s'
         + γ · Σ π(a|s) · Σ P(s'|s,a) · V^π(s')
              a          s'
```

**含义:** 当前价值 = 即时奖励 + 未来价值的折扣

**对 Q^π:**
```
Q^π(s, a) = Σ P(s'|s,a) · [R(s,a,s') + γ · Σ π(a'|s') · Q^π(s', a')]
            s'                           a'
```

---

### 5.2 贝尔曼最优方程

**最优价值函数:**
```
V*(s) = max V^π(s)
        π

Q*(s, a) = max Q^π(s, a)
           π
```

**贝尔曼最优方程:**
```
V*(s) = max Σ P(s'|s,a) · [R(s,a,s') + γ·V*(s')]
        a   s'

Q*(s, a) = Σ P(s'|s,a) · [R(s,a,s') + γ · max Q*(s', a')]
           s'                           a'
```

**最优策略:**
```
π*(s) = argmax Σ P(s'|s,a) · [R(s,a,s') + γ·V*(s')]
        a      s'
```

---

## 6. 动态规划求解 MDP

### 6.1 策略评估(Policy Evaluation)

**目标:** 给定策略 π,计算 V^π

**迭代更新:**
```
V_(k+1)(s) = Σ π(a|s) · Σ P(s'|s,a) · [R(s,a,s') + γ·V_k(s')]
             a          s'
```

**算法:**
```python
def policy_evaluation(policy, P, R, gamma, theta=1e-6):
    """
    policy: π(a|s) - 策略
    P: P(s'|s,a) - 转移概率
    R: R(s,a,s') - 奖励函数
    gamma: 折扣因子
    theta: 收敛阈值
    """
    V = np.zeros(len(S))  # 初始化价值函数

    while True:
        delta = 0
        for s in S:
            v = V[s]
            # 贝尔曼期望更新
            V[s] = sum(
                policy[s][a] * sum(
                    P[s][a][s_next] * (R[s][a][s_next] + gamma * V[s_next])
                    for s_next in S
                )
                for a in A
            )
            delta = max(delta, abs(v - V[s]))

        if delta < theta:
            break

    return V
```

**收敛性:** V_k → V^π (压缩映射定理)

---

### 6.2 价值迭代(Value Iteration)

**目标:** 直接计算 V*

**迭代更新:**
```
V_(k+1)(s) = max Σ P(s'|s,a) · [R(s,a,s') + γ·V_k(s')]
             a   s'
```

**算法:**
```python
def value_iteration(P, R, gamma, theta=1e-6):
    V = np.zeros(len(S))

    while True:
        delta = 0
        for s in S:
            v = V[s]
            # 贝尔曼最优更新
            V[s] = max(
                sum(
                    P[s][a][s_next] * (R[s][a][s_next] + gamma * V[s_next])
                    for s_next in S
                )
                for a in A
            )
            delta = max(delta, abs(v - V[s]))

        if delta < theta:
            break

    # 提取最优策略
    policy = np.zeros(len(S), dtype=int)
    for s in S:
        policy[s] = np.argmax([
            sum(
                P[s][a][s_next] * (R[s][a][s_next] + gamma * V[s_next])
                for s_next in S
            )
            for a in A
        ])

    return V, policy
```

**复杂度:** O(|S|²|A|) per iteration

---

### 6.3 策略迭代(Policy Iteration)

**思想:** 交替执行策略评估和策略改进

**算法:**
```
1. 初始化随机策略 π_0
2. 循环:
   a) 策略评估: 计算 V^π
   b) 策略改进: π'(s) = argmax Q^π(s, a)
   c) 若 π' = π,停止;否则 π ← π'
```

**代码:**
```python
def policy_iteration(P, R, gamma):
    # 随机初始化策略
    policy = np.random.choice(len(A), size=len(S))

    while True:
        # 策略评估
        V = policy_evaluation(policy, P, R, gamma)

        # 策略改进
        policy_stable = True
        for s in S:
            old_action = policy[s]
            # 贪心选择
            policy[s] = np.argmax([
                sum(
                    P[s][a][s_next] * (R[s][a][s_next] + gamma * V[s_next])
                    for s_next in S
                )
                for a in A
            ])
            if old_action != policy[s]:
                policy_stable = False

        if policy_stable:
            break

    return policy, V
```

**优势:** 通常比价值迭代更快(策略空间 < 价值空间)

---

## 7. 实战案例:网格世界(GridWorld)

### 7.1 问题设定

```
起点(S) □ □ 陷阱(T)
   □    □ □   □
   □    □ □ 目标(G)
```

- **状态:** 12 个格子
- **动作:** 上/下/左/右
- **奖励:** 目标 +10, 陷阱 -10, 其他 -1
- **转移:** 确定性(碰壁不动)

### 7.2 完整实现

```python
import numpy as np
import matplotlib.pyplot as plt

class GridWorld:
    def __init__(self):
        self.rows = 3
        self.cols = 4
        self.start = (0, 0)
        self.goal = (2, 3)
        self.trap = (0, 3)

        # 动作: 0=上, 1=下, 2=左, 3=右
        self.actions = [(-1,0), (1,0), (0,-1), (0,1)]
        self.n_actions = len(self.actions)
        self.n_states = self.rows * self.cols

    def state_to_coord(self, state):
        return (state // self.cols, state % self.cols)

    def coord_to_state(self, row, col):
        return row * self.cols + col

    def get_next_state(self, state, action):
        """确定性转移"""
        row, col = self.state_to_coord(state)
        d_row, d_col = self.actions[action]
        new_row = np.clip(row + d_row, 0, self.rows - 1)
        new_col = np.clip(col + d_col, 0, self.cols - 1)
        return self.coord_to_state(new_row, new_col)

    def get_reward(self, state, action, next_state):
        row, col = self.state_to_coord(next_state)
        if (row, col) == self.goal:
            return 10
        elif (row, col) == self.trap:
            return -10
        else:
            return -1

    def is_terminal(self, state):
        row, col = self.state_to_coord(state)
        return (row, col) in [self.goal, self.trap]

def value_iteration_gridworld(env, gamma=0.9, theta=1e-4):
    V = np.zeros(env.n_states)

    iteration = 0
    while True:
        delta = 0
        for s in range(env.n_states):
            if env.is_terminal(s):
                continue

            v = V[s]
            # 贝尔曼最优方程
            action_values = []
            for a in range(env.n_actions):
                s_next = env.get_next_state(s, a)
                r = env.get_reward(s, a, s_next)
                action_values.append(r + gamma * V[s_next])

            V[s] = max(action_values)
            delta = max(delta, abs(v - V[s]))

        iteration += 1
        if delta < theta:
            break

    # 提取策略
    policy = np.zeros(env.n_states, dtype=int)
    for s in range(env.n_states):
        if env.is_terminal(s):
            continue

        action_values = []
        for a in range(env.n_actions):
            s_next = env.get_next_state(s, a)
            r = env.get_reward(s, a, s_next)
            action_values.append(r + gamma * V[s_next])

        policy[s] = np.argmax(action_values)

    print(f"收敛于第 {iteration} 次迭代")
    return V, policy

# 运行
env = GridWorld()
V, policy = value_iteration_gridworld(env)

# 可视化
print("\n价值函数:")
print(V.reshape(env.rows, env.cols))

print("\n策略 (0=上, 1=下, 2=左, 3=右):")
print(policy.reshape(env.rows, env.cols))

# 箭头表示
arrows = ['↑', '↓', '←', '→']
print("\n策略可视化:")
for i in range(env.rows):
    for j in range(env.cols):
        s = env.coord_to_state(i, j)
        if (i, j) == env.goal:
            print(" G ", end="")
        elif (i, j) == env.trap:
            print(" T ", end="")
        else:
            print(f" {arrows[policy[s]]} ", end="")
    print()
```

**输出示例:**
```
收敛于第 15 次迭代

价值函数:
[[ 4.87  5.97  7.3  10.  ]
 [ 3.91  4.77  6.07 -10.  ]
 [ 2.86  3.65  5.03  7.61]]

策略 (0=上, 1=下, 2=左, 3=右):
[[3 3 3 0]
 [0 0 3 0]
 [0 0 3 1]]

策略可视化:
 →  →  →  G
 ↑  ↑  →  T
 ↑  ↑  →  ↓
```

---

## 8. 理论深入

### 8.1 最优性原理

**贝尔曼最优性原理:**
最优策略的子策略也是最优的。

**数学表述:**
若 π* 从 s 到 s' 是最优的,则 π* 从 s' 开始也是最优的。

**应用:** 动态规划的基础

---

### 8.2 价值迭代的收敛性

**定理:** 价值迭代算子 T 是 γ-压缩映射:
```
||T(V1) - T(V2)||_∞ ≤ γ · ||V1 - V2||_∞
```

**推论:**
1. 存在唯一不动点 V*
2. 从任意 V0 开始,V_k → V*
3. 收敛速率: O(γ^k)

**证明思路:**
```
|T(V1)(s) - T(V2)(s)|
= |max_a E[r + γV1(s')] - max_a E[r + γV2(s')]|
≤ max_a |E[γV1(s') - γV2(s')]|
≤ γ · max_s |V1(s) - V2(s)|
```

---

### 8.3 策略迭代的收敛性

**定理:** 策略改进单调性
```
若 Q^π(s, π'(s)) ≥ V^π(s) ∀s, 则 V^π' ≥ V^π
```

**推论:** 策略迭代在有限步内收敛到 π*

---

## 9. MDP 的扩展

### 9.1 连续状态空间

**问题:** 状态无限,无法枚举

**解决方案:**
1. **离散化:** 网格划分
2. **函数近似:** V(s; θ) (神经网络)
3. **核方法:** 基于相似度

### 9.2 部分可观测 MDP (POMDP)

**设定:** Agent 无法直接观测状态,只能观测 o_t

**数学模型:**
```
POMDP = (S, A, P, R, Ω, O, γ)
```
- Ω: 观测空间
- O: 观测概率 O(o|s, a)

**解法:** 维护信念状态 b(s) = P(s_t = s | o_1, ..., o_t)

### 9.3 多智能体 MDP

**马尔可夫博弈(Markov Game):**
```
MG = (S, A1, ..., An, P, R1, ..., Rn, γ)
```

**纳什均衡:** 无人能单方面改进

---

## 10. 实践技巧

### 10.1 探索 vs 利用困境

**问题:** 如何平衡已知好动作和探索新动作?

**策略:**
- **ε-greedy:** 以 ε 概率随机探索
- **Softmax:** π(a|s) ∝ exp(Q(s,a)/τ)
- **UCB:** 选择 a = argmax[Q(s,a) + c·√(log N(s)/N(s,a))]

### 10.2 奖励塑形(Reward Shaping)

**技巧:** 添加辅助奖励加速学习
```
R'(s, a, s') = R(s, a, s') + F(s, s')
```

**风险:** 可能改变最优策略

**安全方法 - Potential-Based Shaping:**
```
F(s, s') = γ·Φ(s') - Φ(s)
```
保证最优策略不变!

### 10.3 经验回放

**问题:** RL 数据序列相关,破坏 i.i.d. 假设

**解决:** 存储经验 (s, a, r, s'),随机采样训练

---

## 11. 从 MDP 到深度强化学习

### 11.1 价值函数近似

**大规模状态空间:** 用神经网络近似
```
V(s) ≈ V(s; θ)
Q(s, a) ≈ Q(s, a; θ)
```

**训练:** 最小化 TD 误差
```
L(θ) = E[(r + γ·V(s'; θ) - V(s; θ))²]
```

### 11.2 策略梯度

**直接优化策略:**
```
π(a|s; θ)
```

**目标函数:**
```
J(θ) = E_π[G_t]
```

**梯度:**
```
∇J(θ) = E_π[∇log π(a|s; θ) · Q^π(s, a)]
```

### 11.3 Actor-Critic

**结合:** 策略网络(Actor) + 价值网络(Critic)
```
Actor:  π(a|s; θ)
Critic: V(s; w)
```

**优势:** 低方差的策略梯度

---

## 12. 总结

### 核心概念
- **MDP:** 强化学习的数学基础
- **价值函数:** 评估状态/动作的好坏
- **贝尔曼方程:** 递归关系的精髓
- **动态规划:** 模型已知时的精确解法

### 关键洞察
1. **折扣因子平衡当前与未来**
2. **价值迭代 vs 策略迭代:** 速度与稳定性权衡
3. **探索-利用:** RL 的永恒主题

### 进阶方向
- **无模型 RL:** Q-Learning, SARSA
- **深度 RL:** DQN, A3C, PPO, SAC
- **多智能体 RL:** MARL, Self-Play
- **逆强化学习:** 从演示中学习奖励

### 应用领域
- 游戏 AI (AlphaGo, OpenAI Five)
- 机器人控制
- 推荐系统
- 自动驾驶
- 资源调度

**记住:** MDP 是理解现代强化学习算法的钥匙!
