# PyTorchæ·±åº¦æ•™ç¨‹ï¼ˆä¸€ï¼‰ï¼šå¿«é€Ÿå…¥é—¨ä¸æ ¸å¿ƒæ¦‚å¿µ

> **ç›®æ ‡è¯»è€…**ï¼šå…·æœ‰PythonåŸºç¡€ï¼Œè¿½æ±‚åå¹´ç»éªŒçº§åˆ«çš„æ·±åº¦ç†è§£
> **å­¦ä¹ è·¯å¾„**ï¼šä»å®è·µå‡ºå‘ï¼Œåœ¨éœ€è¦æ—¶è¡¥å……æ•°å­¦åŸç†

---

## ç¬¬ä¸€éƒ¨åˆ†ï¼š5åˆ†é’Ÿå¿«é€Ÿä¸Šæ‰‹

### 1.1 ç¬¬ä¸€ä¸ªç¥ç»ç½‘ç»œ

```python
import torch
import torch.nn as nn
import torch.optim as optim

# 1. åˆ›å»ºä¸€ä¸ªç®€å•çš„çº¿æ€§å›å½’æ¨¡å‹
model = nn.Linear(in_features=1, out_features=1)

# 2. å‡†å¤‡æ•°æ®ï¼ˆy = 2x + 3ï¼‰
x_train = torch.tensor([[1.0], [2.0], [3.0], [4.0]])
y_train = torch.tensor([[5.0], [7.0], [9.0], [11.0]])

# 3. å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
criterion = nn.MSELoss()
optimizer = optim.SGD(model.parameters(), lr=0.01)

# 4. è®­ç»ƒå¾ªç¯
for epoch in range(100):
    # å‰å‘ä¼ æ’­
    y_pred = model(x_train)

    # è®¡ç®—æŸå¤±
    loss = criterion(y_pred, y_train)

    # åå‘ä¼ æ’­
    optimizer.zero_grad()  # æ¸…ç©ºæ¢¯åº¦
    loss.backward()         # è®¡ç®—æ¢¯åº¦
    optimizer.step()        # æ›´æ–°å‚æ•°

    if (epoch + 1) % 20 == 0:
        print(f'Epoch [{epoch+1}/100], Loss: {loss.item():.4f}')

# 5. æµ‹è¯•æ¨¡å‹
with torch.no_grad():
    test_x = torch.tensor([[5.0]])
    test_y = model(test_x)
    print(f'Input: 5.0, Predicted: {test_y.item():.2f}, Expected: ~13.0')
```

**è¿è¡Œç»“æœåˆ†æ**ï¼š
- æ¨¡å‹å­¦ä¹ åˆ°äº†çº¿æ€§å…³ç³» y â‰ˆ 2x + 3
- æŸå¤±å‡½æ•°é€æ¸é™ä½ï¼Œè¯´æ˜æ¨¡å‹åœ¨ä¼˜åŒ–
- æœ€ç»ˆé¢„æµ‹ç»“æœæ¥è¿‘çœŸå®å€¼

---

## ç¬¬äºŒéƒ¨åˆ†ï¼šPyTorchæ ¸å¿ƒæ¶æ„

### 2.1 å¼ é‡ï¼ˆTensorï¼‰ï¼šPyTorchçš„æ ¸å¿ƒæ•°æ®ç»“æ„

#### 2.1.1 ä»€ä¹ˆæ˜¯å¼ é‡ï¼Ÿ

**ç®€å•ç†è§£**ï¼š
- 0ç»´å¼ é‡ = æ ‡é‡ï¼ˆä¸€ä¸ªæ•°å­—ï¼‰ï¼š`torch.tensor(3.14)`
- 1ç»´å¼ é‡ = å‘é‡ï¼ˆä¸€ä¸²æ•°å­—ï¼‰ï¼š`torch.tensor([1, 2, 3])`
- 2ç»´å¼ é‡ = çŸ©é˜µï¼ˆè¡¨æ ¼ï¼‰ï¼š`torch.tensor([[1, 2], [3, 4]])`
- 3ç»´å¼ é‡ = ç«‹æ–¹ä½“ï¼ˆå¦‚RGBå›¾åƒï¼‰ï¼š`torch.tensor([[[...]]])`

```python
# åˆ›å»ºå¼ é‡çš„å¤šç§æ–¹å¼
import torch

# 1. ä»Pythonåˆ—è¡¨åˆ›å»º
a = torch.tensor([1, 2, 3])
print(a)  # tensor([1, 2, 3])

# 2. åˆ›å»ºç‰¹æ®Šå¼ é‡
zeros = torch.zeros(2, 3)        # å…¨0çŸ©é˜µ
ones = torch.ones(2, 3)          # å…¨1çŸ©é˜µ
rand = torch.randn(2, 3)         # æ ‡å‡†æ­£æ€åˆ†å¸ƒéšæœºæ•°
arange = torch.arange(0, 10, 2)  # [0, 2, 4, 6, 8]

# 3. æŸ¥çœ‹å¼ é‡å±æ€§
print(f"å½¢çŠ¶: {rand.shape}")      # torch.Size([2, 3])
print(f"æ•°æ®ç±»å‹: {rand.dtype}")  # torch.float32
print(f"è®¾å¤‡: {rand.device}")     # cpu æˆ– cuda:0
```

#### 2.1.2 å¼ é‡çš„åŸºæœ¬è¿ç®—

```python
# åŸºç¡€è¿ç®—
x = torch.tensor([[1.0, 2.0], [3.0, 4.0]])
y = torch.tensor([[5.0, 6.0], [7.0, 8.0]])

# é€å…ƒç´ è¿ç®—
z1 = x + y      # åŠ æ³•ï¼š[[6, 8], [10, 12]]
z2 = x * y      # é€å…ƒç´ ä¹˜æ³•ï¼š[[5, 12], [21, 32]]
z3 = x / y      # é™¤æ³•
z4 = x ** 2     # å¹³æ–¹

# çŸ©é˜µè¿ç®—
z5 = x @ y      # çŸ©é˜µä¹˜æ³•
z6 = x.T        # è½¬ç½®

# èšåˆè¿ç®—
print(x.sum())      # æ‰€æœ‰å…ƒç´ æ±‚å’Œï¼š10.0
print(x.mean())     # å¹³å‡å€¼ï¼š2.5
print(x.max())      # æœ€å¤§å€¼ï¼š4.0
print(x.argmax())   # æœ€å¤§å€¼ç´¢å¼•ï¼š3
```

<details>
<summary>ğŸ“ æ•°å­¦è¡¥å……ï¼šçŸ©é˜µä¹˜æ³•çš„æ•°å­¦åŸç†ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

**ğŸ’¡ è®°å¿†å£è¯€**ï¼š
```
è¡Œä¹˜åˆ—ï¼Œå¯¹åº”ä¹˜ï¼Œæ±‚å’Œå¡«æ–°ä½
å·¦è¡Œæ•°ï¼Œå³åˆ—æ•°ï¼Œä¸­é—´è¦ç›¸ç­‰
```

**çŸ©é˜µä¹˜æ³•å®šä¹‰**ï¼š
```
ç»™å®š A(mÃ—n) å’Œ B(nÃ—p)ï¼Œç»“æœ C(mÃ—p) çš„è®¡ç®—ï¼š
C[i,j] = Î£(k=1 to n) A[i,k] * B[k,j]

æ„æ€ï¼šå·¦è¾¹ç¬¬iè¡Œï¼Œä¹˜ä»¥å³è¾¹ç¬¬jåˆ—ï¼Œå¯¹åº”ä½ç½®ç›¸ä¹˜å†æ±‚å’Œ
```

**æ‰‹å·¥è®¡ç®—ç¤ºä¾‹**ï¼š
```
A = [1  2]    B = [5  6]
    [3  4]        [7  8]

AB = [(1Ã—5 + 2Ã—7)  (1Ã—6 + 2Ã—8)]   [19  22]
     [(3Ã—5 + 4Ã—7)  (3Ã—6 + 4Ã—8)] = [43  50]

ç¬¬1è¡Œç¬¬1åˆ—: Açš„ç¬¬1è¡Œ Ã— Bçš„ç¬¬1åˆ— = 1Ã—5 + 2Ã—7 = 19
ç¬¬1è¡Œç¬¬2åˆ—: Açš„ç¬¬1è¡Œ Ã— Bçš„ç¬¬2åˆ— = 1Ã—6 + 2Ã—8 = 22
```

**PyTorchéªŒè¯**ï¼š
```python
A = torch.tensor([[1., 2.], [3., 4.]])
B = torch.tensor([[5., 6.], [7., 8.]])
print(A @ B)  # tensor([[19., 22.], [43., 50.]])
```

**å‡ ä½•æ„ä¹‰**ï¼š
- çŸ©é˜µä¹˜æ³• = çº¿æ€§å˜æ¢çš„ç»„åˆ
- A @ xï¼šç”¨çŸ©é˜µAå˜æ¢å‘é‡x
- ç¥ç»ç½‘ç»œçš„æ¯ä¸€å±‚æœ¬è´¨ä¸Šå°±æ˜¯çŸ©é˜µä¹˜æ³• + éçº¿æ€§æ¿€æ´»

</details>

#### 2.1.3 å†…å­˜å¸ƒå±€ä¸å­˜å‚¨æœºåˆ¶

```python
class TensorStorageSystem:
    """
    æ·±å…¥ç†è§£PyTorchå¼ é‡çš„å­˜å‚¨ç³»ç»Ÿ
    """

    def storage_and_view(self):
        """
        å­˜å‚¨ï¼ˆStorageï¼‰ä¸è§†å›¾ï¼ˆViewï¼‰æœºåˆ¶
        """
        # 1. å­˜å‚¨æ˜¯ä¸€ç»´è¿ç»­å†…å­˜å—
        x = torch.tensor([[1, 2, 3], [4, 5, 6]])
        print(f"å­˜å‚¨å†…å®¹: {x.storage()}")  # [1, 2, 3, 4, 5, 6]

        # 2. å¼ é‡æ˜¯å­˜å‚¨çš„è§†å›¾
        # é€šè¿‡offset, size, strideå®šä¹‰
        print(f"åç§»é‡: {x.storage_offset()}")
        print(f"å½¢çŠ¶: {x.size()}")
        print(f"æ­¥å¹…: {x.stride()}")  # (3, 1) è¡¨ç¤ºè¡Œé—´éš”3ï¼Œåˆ—é—´éš”1

        # 3. è§†å›¾æ“ä½œé›¶æ‹·è´
        y = x.view(3, 2)  # åªæ”¹å˜å½¢çŠ¶å…ƒæ•°æ®
        z = x.t()          # è½¬ç½®åªæ”¹å˜æ­¥å¹…

        # 4. æ­¥å¹…çš„æ•°å­¦æ„ä¹‰
        # å…ƒç´ x[i,j]çš„å†…å­˜åœ°å€ï¼š
        # addr = storage_offset + i*stride[0] + j*stride[1]

    def advanced_indexing(self):
        """
        é«˜çº§ç´¢å¼•ä¸å†…å­˜è¿ç»­æ€§
        """
        x = torch.randn(10, 20, 30)

        # åŸºç¡€ç´¢å¼•ï¼ˆä¸å¤åˆ¶ï¼‰
        y1 = x[0]         # shape: (20, 30)
        y2 = x[:, 0, :]   # shape: (10, 30)

        # é«˜çº§ç´¢å¼•ï¼ˆå¤åˆ¶æ•°æ®ï¼‰
        indices = torch.tensor([0, 2, 4])
        y3 = x[indices]   # éœ€è¦å¤åˆ¶ï¼Œå› ä¸ºç´¢å¼•ä¸è¿ç»­

        # æ©ç ç´¢å¼•
        mask = x > 0
        y4 = x[mask]      # è¿”å›ä¸€ç»´å¼ é‡

    def memory_format(self):
        """
        å†…å­˜æ ¼å¼ï¼šNCHW vs NHWC
        """
        # Channels Lastæ ¼å¼ï¼ˆå¯¹æŸäº›ç¡¬ä»¶æ›´ä¼˜ï¼‰
        x = torch.randn(8, 3, 224, 224)  # NCHW
        x_cl = x.to(memory_format=torch.channels_last)  # NHWC

        # æ£€æŸ¥å†…å­˜æ ¼å¼
        print(f"æ˜¯å¦channels_last: {x_cl.is_contiguous(memory_format=torch.channels_last)}")

# å®æˆ˜æ¼”ç¤º
demo = TensorStorageSystem()
demo.storage_and_view()
```

**ä¸ºä»€ä¹ˆç†è§£å†…å­˜å¸ƒå±€å¾ˆé‡è¦ï¼Ÿ**
1. **æ€§èƒ½ä¼˜åŒ–**ï¼šè¿ç»­å†…å­˜è®¿é—®æ›´å¿«
2. **é¿å…æ„å¤–bug**ï¼šç†è§£ä½•æ—¶å‘ç”Ÿæ•°æ®å¤åˆ¶
3. **GPUä¼˜åŒ–**ï¼šä¸åŒå†…å­˜æ ¼å¼å¯¹GPUæ€§èƒ½å½±å“å·¨å¤§

---

### 2.2 è‡ªåŠ¨å¾®åˆ†ï¼ˆAutogradï¼‰ï¼šåå‘ä¼ æ’­çš„é­”æ³•

#### 2.2.1 æ¢¯åº¦è®¡ç®—å…¥é—¨

```python
# å¼€å¯æ¢¯åº¦è¿½è¸ª
x = torch.tensor(2.0, requires_grad=True)
y = x ** 2 + 3 * x + 1

# è®¡ç®—æ¢¯åº¦
y.backward()

print(f"x = {x.item()}")           # 2.0
print(f"y = {y.item()}")           # 4 + 6 + 1 = 11
print(f"dy/dx = {x.grad.item()}")  # 2*2 + 3 = 7
```

<details>
<summary>ğŸ“ æ•°å­¦è¡¥å……ï¼šæ¢¯åº¦çš„å«ä¹‰ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

**ğŸ’¡ è®°å¿†å£è¯€**ï¼š
```
æ¢¯åº¦å°±æ˜¯æ–œç‡å¡ï¼Œå‘Šè¯‰æ–¹å‘æ€ä¹ˆèµ°
è‡ªå˜é‡å˜ä¸€å°æ­¥ï¼Œå› å˜é‡è·Ÿç€èµ°
```

**æ¢¯åº¦å®šä¹‰**ï¼š
```
å¯¹äºå‡½æ•° y = f(x)ï¼Œæ¢¯åº¦ dy/dx è¡¨ç¤ºï¼š
- xå˜åŒ–ä¸€ä¸ªå°é‡Î”xæ—¶ï¼Œyå˜åŒ–å¤šå°‘
- å‡½æ•°åœ¨è¯¥ç‚¹çš„æ–œç‡
- ä¼˜åŒ–æ—¶çš„å‰è¿›æ–¹å‘ï¼ˆè´Ÿæ¢¯åº¦ = ä¸‹é™æœ€å¿«ï¼‰
```

**ä¾‹å­**ï¼šy = xÂ² + 3x + 1
```
dy/dx = 2x + 3

å½“ x = 2ï¼š
dy/dx = 2(2) + 3 = 7

å«ä¹‰ï¼šxä»2å¢åŠ åˆ°2.01æ—¶ï¼Œ
     yå¤§çº¦å¢åŠ  7 Ã— 0.01 = 0.07
```

**éªŒè¯**ï¼š
```python
x = 2.0
y1 = x**2 + 3*x + 1  # 11.0
x = 2.01
y2 = x**2 + 3*x + 1  # 11.0701
print(f"å®é™…å˜åŒ–: {y2 - y1:.4f}")      # 0.0701
print(f"æ¢¯åº¦é¢„æµ‹: {7 * 0.01:.4f}")     # 0.0700
```

</details>

#### 2.2.2 å¤šå˜é‡æ¢¯åº¦

```python
# å¤šå˜é‡å‡½æ•°
x = torch.tensor([1.0, 2.0], requires_grad=True)
y = x[0]**2 + 3*x[0]*x[1] + x[1]**2

y.backward()

print(f"âˆ‚y/âˆ‚xâ‚€ = {x.grad[0].item()}")  # 2*1 + 3*2 = 8
print(f"âˆ‚y/âˆ‚xâ‚ = {x.grad[1].item()}")  # 3*1 + 2*2 = 7
```

#### 2.2.3 è®¡ç®—å›¾ä¸åå‘ä¼ æ’­

```python
class ComputationGraphDemo:
    """
    ç†è§£PyTorchçš„åŠ¨æ€è®¡ç®—å›¾
    """

    def visualize_graph(self):
        """
        å¯è§†åŒ–è®¡ç®—å›¾
        """
        x = torch.tensor(2.0, requires_grad=True)
        y = torch.tensor(3.0, requires_grad=True)

        # æ„å»ºè®¡ç®—å›¾
        a = x + y       # a = 5.0
        b = x * y       # b = 6.0
        c = a * b       # c = 30.0

        c.backward()

        print(f"dc/dx = {x.grad}")  # 11.0
        print(f"dc/dy = {y.grad}")  # 13.0

        """
        è®¡ç®—å›¾ï¼š

        x=2 â”€â”¬â”€(+)â”€â†’ a=5 â”€â”
             â”‚            â”œâ”€(*)â”€â†’ c=30
        y=3 â”€â”¼â”€(*)â”€â†’ b=6 â”€â”˜
             â”‚

        åå‘ä¼ æ’­ï¼š
        dc/dx = dc/da * da/dx + dc/db * db/dx
              = b * 1 + a * y
              = 6 + 5*1 = 11
        """

    def gradient_accumulation(self):
        """
        æ¢¯åº¦ç´¯ç§¯æœºåˆ¶
        """
        x = torch.tensor(1.0, requires_grad=True)

        # ç¬¬ä¸€æ¬¡å‰å‘+åå‘
        y1 = x ** 2
        y1.backward()
        print(f"ç¬¬ä¸€æ¬¡æ¢¯åº¦: {x.grad}")  # 2.0

        # ä¸æ¸…é›¶ï¼Œå†æ¬¡åå‘ï¼ˆæ¢¯åº¦ä¼šç´¯ç§¯ï¼ï¼‰
        y2 = x ** 3
        y2.backward()
        print(f"ç´¯ç§¯åæ¢¯åº¦: {x.grad}")  # 2.0 + 3.0 = 5.0

        # è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè®­ç»ƒæ—¶éœ€è¦ optimizer.zero_grad()

# å®æˆ˜æ¼”ç¤º
demo = ComputationGraphDemo()
demo.visualize_graph()
demo.gradient_accumulation()
```

<details>
<summary>ğŸ“ æ•°å­¦è¡¥å……ï¼šé“¾å¼æ³•åˆ™ä¸åå‘ä¼ æ’­ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

**ğŸ’¡ è®°å¿†å£è¯€**ï¼š
```
é“¾å¼æ±‚å¯¼åƒæ¥åŠ›ï¼Œä¸€æ£’ä¸€æ£’å¾€å›é€’
å¤–å±‚å¯¼æ•°ä¹˜å†…å±‚ï¼Œä»åå¾€å‰å…¨è¿èµ·
```

**é“¾å¼æ³•åˆ™ï¼ˆChain Ruleï¼‰**ï¼š
```
å¦‚æœ z = f(y), y = g(x)ï¼Œé‚£ä¹ˆï¼š
dz/dx = (dz/dy) Ã— (dy/dx)

æ„æ€ï¼šzå¯¹xçš„å¯¼æ•° = zå¯¹yçš„å¯¼æ•° Ã— yå¯¹xçš„å¯¼æ•°
å°±åƒæ¥åŠ›èµ›ï¼Œæ¢¯åº¦ä»åå¾€å‰ä¸€å±‚å±‚ä¼ 
```

**åå‘ä¼ æ’­æœ¬è´¨**ï¼š
- å°±æ˜¯é“¾å¼æ³•åˆ™çš„é€’å½’åº”ç”¨
- ä»è¾“å‡ºèŠ‚ç‚¹å¼€å§‹ï¼Œé€å±‚å‘åè®¡ç®—æ¢¯åº¦
- ç¥ç»ç½‘ç»œè®­ç»ƒçš„æ ¸å¿ƒç®—æ³•

**ä¾‹å­**ï¼šz = (xÂ² + 1)Â³
```
è®¾ y = xÂ² + 1, z = yÂ³

å‰å‘ä¼ æ’­ï¼ˆä»å·¦åˆ°å³ç®—å€¼ï¼‰ï¼š
x = 2 â†’ y = 5 â†’ z = 125

åå‘ä¼ æ’­ï¼ˆä»å³åˆ°å·¦ç®—æ¢¯åº¦ï¼‰ï¼š
dz/dy = 3yÂ² = 75
dy/dx = 2x = 4
dz/dx = (dz/dy) Ã— (dy/dx) = 75 Ã— 4 = 300
```

**PyTorchéªŒè¯**ï¼š
```python
x = torch.tensor(2.0, requires_grad=True)
z = (x**2 + 1)**3
z.backward()
print(x.grad)  # tensor(300.)
```

</details>

---

### 2.3 ç¥ç»ç½‘ç»œæ¨¡å—ï¼ˆnn.Moduleï¼‰

#### 2.3.1 æ„å»ºè‡ªå®šä¹‰ç½‘ç»œ

```python
import torch.nn as nn
import torch.nn.functional as F

class SimpleNN(nn.Module):
    def __init__(self, input_size, hidden_size, output_size):
        super(SimpleNN, self).__init__()

        # å®šä¹‰å±‚
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        # å‰å‘ä¼ æ’­é€»è¾‘
        x = F.relu(self.fc1(x))  # éšè—å±‚ + ReLUæ¿€æ´»
        x = self.fc2(x)           # è¾“å‡ºå±‚
        return x

# åˆ›å»ºæ¨¡å‹
model = SimpleNN(input_size=10, hidden_size=20, output_size=2)

# æŸ¥çœ‹æ¨¡å‹ç»“æ„
print(model)

# æŸ¥çœ‹å‚æ•°
for name, param in model.named_parameters():
    print(f"{name}: {param.shape}")
```

**è¾“å‡ºè§£é‡Š**ï¼š
```
SimpleNN(
  (fc1): Linear(in_features=10, out_features=20, bias=True)
  (fc2): Linear(in_features=20, out_features=2, bias=True)
)

fc1.weight: torch.Size([20, 10])  # ç¬¬ä¸€å±‚æƒé‡çŸ©é˜µ
fc1.bias: torch.Size([20])         # ç¬¬ä¸€å±‚åç½®
fc2.weight: torch.Size([2, 20])    # ç¬¬äºŒå±‚æƒé‡çŸ©é˜µ
fc2.bias: torch.Size([2])          # ç¬¬äºŒå±‚åç½®
```

#### 2.3.2 æ¿€æ´»å‡½æ•°

```python
# å¸¸ç”¨æ¿€æ´»å‡½æ•°
x = torch.linspace(-5, 5, 100)

# ReLU: max(0, x)
relu_out = F.relu(x)

# Sigmoid: 1 / (1 + e^(-x))
sigmoid_out = torch.sigmoid(x)

# Tanh: (e^x - e^(-x)) / (e^x + e^(-x))
tanh_out = torch.tanh(x)

# LeakyReLU: max(0.01x, x)
leaky_relu_out = F.leaky_relu(x, 0.01)

# GELU (ç°ä»£Transformerå¸¸ç”¨)
gelu_out = F.gelu(x)
```

**ä¸ºä»€ä¹ˆéœ€è¦æ¿€æ´»å‡½æ•°ï¼Ÿ**
- æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼Œå¤šå±‚ç¥ç»ç½‘ç»œ = ä¸€ä¸ªçº¿æ€§å˜æ¢
- æ¿€æ´»å‡½æ•°å¼•å…¥éçº¿æ€§ï¼Œè®©ç½‘ç»œèƒ½æ‹Ÿåˆå¤æ‚å‡½æ•°

<details>
<summary>ğŸ“ æ•°å­¦è¡¥å……ï¼šä¸ºä»€ä¹ˆçº¿æ€§å±‚å åŠ è¿˜æ˜¯çº¿æ€§ï¼Ÿï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

**ğŸ’¡ è®°å¿†å£è¯€**ï¼š
```
çº¿æ€§å¥—çº¿æ€§ï¼Œè¿˜æ˜¯çº¿æ€§å˜
åŠ ä¸ªæ¿€æ´»å‡½æ•°ï¼Œæ‰èƒ½æ‹å¼¯å¼¯
```

**è¯æ˜**ï¼š
```
å‡è®¾ä¸¤å±‚çº¿æ€§å˜æ¢ï¼š
h = Wâ‚x + bâ‚
y = Wâ‚‚h + bâ‚‚

ä»£å…¥ï¼š
y = Wâ‚‚(Wâ‚x + bâ‚) + bâ‚‚
  = Wâ‚‚Wâ‚x + Wâ‚‚bâ‚ + bâ‚‚
  = Wx + b  ï¼ˆå…¶ä¸­ W = Wâ‚‚Wâ‚, b = Wâ‚‚bâ‚ + bâ‚‚ï¼‰

ç»“è®ºï¼šä¸¤å±‚çº¿æ€§å˜æ¢ç­‰ä»·äºä¸€å±‚ï¼
å°±åƒ y = 2(3x + 1) + 5 = 6x + 7ï¼Œæœ€ç»ˆè¿˜æ˜¯ä¸€æ¡ç›´çº¿
```

**åŠ å…¥æ¿€æ´»å‡½æ•°å**ï¼š
```
h = Ïƒ(Wâ‚x + bâ‚)   # Ïƒæ˜¯æ¿€æ´»å‡½æ•°ï¼ˆå¦‚ReLUï¼‰
y = Wâ‚‚h + bâ‚‚

æ­¤æ—¶æ— æ³•ç®€åŒ–ä¸ºå•å±‚ï¼Œå› ä¸ºÏƒæ˜¯éçº¿æ€§çš„
å°±åƒå…ˆæŠ˜å å†æ‹‰ä¼¸ï¼Œèƒ½æ‹Ÿåˆå¤æ‚æ›²çº¿
```

**ç›®çš„**ï¼š
- æ²¡æœ‰æ¿€æ´»å‡½æ•°ï¼šå¤šå±‚ç½‘ç»œ = æµªè´¹è®¡ç®—
- æœ‰æ¿€æ´»å‡½æ•°ï¼šèƒ½å­¦ä¹ å¤æ‚çš„éçº¿æ€§æ¨¡å¼

</details>

---

### 2.4 è®¾å¤‡ç®¡ç†ï¼ˆCPU vs GPUï¼‰

```python
# æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# å°†å¼ é‡ç§»åˆ°GPU
x_cpu = torch.randn(1000, 1000)
x_gpu = x_cpu.to(device)

# å°†æ¨¡å‹ç§»åˆ°GPU
model = SimpleNN(10, 20, 2)
model = model.to(device)

# å®Œæ•´è®­ç»ƒå¾ªç¯ç¤ºä¾‹
def train_on_gpu(model, data_loader, optimizer, criterion, device):
    model.train()
    for batch_x, batch_y in data_loader:
        # æ•°æ®ç§»åˆ°GPU
        batch_x = batch_x.to(device)
        batch_y = batch_y.to(device)

        # å‰å‘ä¼ æ’­
        outputs = model(batch_x)
        loss = criterion(outputs, batch_y)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
```

**æ€§èƒ½å¯¹æ¯”**ï¼š
```python
import time

# CPUç‰ˆæœ¬
x = torch.randn(5000, 5000)
y = torch.randn(5000, 5000)

start = time.time()
z = x @ y
print(f"CPUè€—æ—¶: {time.time() - start:.4f}ç§’")

# GPUç‰ˆæœ¬ï¼ˆå¦‚æœå¯ç”¨ï¼‰
if torch.cuda.is_available():
    x_gpu = x.to('cuda')
    y_gpu = y.to('cuda')

    start = time.time()
    z_gpu = x_gpu @ y_gpu
    torch.cuda.synchronize()  # ç­‰å¾…GPUè®¡ç®—å®Œæˆ
    print(f"GPUè€—æ—¶: {time.time() - start:.4f}ç§’")
```

---

## ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ•°å€¼ç¨³å®šæ€§ä¸å®è·µæŠ€å·§

### 3.1 æ•°å€¼ç¨³å®šæ€§é—®é¢˜

#### 3.1.1 æŒ‡æ•°å‡½æ•°çš„æ•°å€¼ç¨³å®šå®ç°

```python
def softmax_naive(x):
    """
    æœ´ç´ å®ç°ï¼ˆæ•°å€¼ä¸ç¨³å®šï¼‰
    """
    exp_x = torch.exp(x)
    return exp_x / exp_x.sum(dim=-1, keepdim=True)

def softmax_stable(x):
    """
    æ•°å€¼ç¨³å®šç‰ˆæœ¬
    """
    # å‡å»æœ€å¤§å€¼ï¼Œé˜²æ­¢æŒ‡æ•°æº¢å‡º
    x_max = x.max(dim=-1, keepdim=True)[0]
    exp_x = torch.exp(x - x_max)
    return exp_x / exp_x.sum(dim=-1, keepdim=True)

# æµ‹è¯•
x = torch.tensor([1000., 1001., 1002.])

try:
    result1 = softmax_naive(x)
    print(f"æœ´ç´ ç‰ˆæœ¬: {result1}")  # å¯èƒ½å‡ºç°NaN
except:
    print("æœ´ç´ ç‰ˆæœ¬æº¢å‡ºï¼")

result2 = softmax_stable(x)
print(f"ç¨³å®šç‰ˆæœ¬: {result2}")  # tensor([0.0900, 0.2447, 0.6652])
```

<details>
<summary>ğŸ“ æ•°å­¦è¡¥å……ï¼šSoftmaxæ•°å€¼ç¨³å®šæ€§ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

**ğŸ’¡ è®°å¿†å£è¯€**ï¼š
```
æŒ‡æ•°å‡½æ•°æ˜“çˆ†ç‚¸ï¼Œå‡å»æœ€å¤§ä¿å¹³å®‰
ä¸Šä¸‹åŒé™¤ä¸æ”¹å€¼ï¼Œç¨³å®šè®¡ç®—æ˜¯å…³é”®
```

**Softmaxå®šä¹‰**ï¼š
```
softmax(xáµ¢) = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)

ä½œç”¨ï¼šæŠŠä¸€ç»„æ•°å­—è½¬æ¢æˆæ¦‚ç‡ï¼ˆå’Œä¸º1ï¼Œéƒ½åœ¨0-1ä¹‹é—´ï¼‰
```

**é—®é¢˜**ï¼šå½“xå¾ˆå¤§æ—¶ï¼Œexp(x)ä¼šæº¢å‡º
```
exp(1000) = âˆ ï¼ˆè¶…å‡ºæµ®ç‚¹æ•°èŒƒå›´ï¼‰
```

**è§£å†³æ–¹æ¡ˆ**ï¼šåˆ©ç”¨æ•°å­¦æ’ç­‰å¼
```
softmax(x) = softmax(x - c)  å¯¹ä»»æ„å¸¸æ•°cæˆç«‹

è¯æ˜ï¼š
softmax(xáµ¢ - c) = exp(xáµ¢ - c) / Î£â±¼ exp(xâ±¼ - c)
                = [exp(xáµ¢) / exp(c)] / [Î£â±¼ exp(xâ±¼) / exp(c)]
                = exp(xáµ¢) / Î£â±¼ exp(xâ±¼)
                = softmax(xáµ¢)

é€‰æ‹© c = max(x)ï¼Œå¯ä»¥ä¿è¯ï¼š
- æ‰€æœ‰ exp(xáµ¢ - c) â‰¤ 1ï¼ˆé˜²æ­¢æº¢å‡ºï¼‰
- è‡³å°‘ä¸€ä¸ª exp(xáµ¢ - c) = 1ï¼ˆé˜²æ­¢ä¸‹æº¢ï¼‰
```

**ç›®çš„**ï¼š
- æœ´ç´ å®ç°ï¼šexp(å¤§æ•°) â†’ æº¢å‡º â†’ NaN
- ç¨³å®šå®ç°ï¼šå‡æœ€å¤§å€¼ â†’ æ•°å€¼åœ¨å®‰å…¨èŒƒå›´ â†’ æ­£ç¡®ç»“æœ

</details>

#### 3.1.2 æ¢¯åº¦æ¶ˆå¤±ä¸çˆ†ç‚¸

```python
class GradientFlowDemo:
    """
    æ¼”ç¤ºæ¢¯åº¦æ¶ˆå¤±å’Œæ¢¯åº¦çˆ†ç‚¸
    """

    def gradient_vanishing(self):
        """
        æ¢¯åº¦æ¶ˆå¤±ç¤ºä¾‹
        """
        # ä½¿ç”¨Sigmoidæ¿€æ´»å‡½æ•°çš„æ·±å±‚ç½‘ç»œ
        x = torch.randn(1, 10, requires_grad=True)

        # 10å±‚ç½‘ç»œï¼Œæ¯å±‚éƒ½ç”¨Sigmoid
        h = x
        for _ in range(10):
            W = torch.randn(10, 10) * 0.5
            h = torch.sigmoid(h @ W)

        loss = h.sum()
        loss.backward()

        print(f"è¾“å…¥æ¢¯åº¦èŒƒæ•°: {x.grad.norm().item():.6f}")
        # å¾ˆå°çš„å€¼ï¼Œè¯´æ˜æ¢¯åº¦æ¶ˆå¤±äº†

    def gradient_exploding(self):
        """
        æ¢¯åº¦çˆ†ç‚¸ç¤ºä¾‹
        """
        x = torch.randn(1, 10, requires_grad=True)

        # æƒé‡è¿‡å¤§å¯¼è‡´æ¢¯åº¦çˆ†ç‚¸
        h = x
        for _ in range(10):
            W = torch.randn(10, 10) * 2  # æƒé‡è¾ƒå¤§
            h = h @ W

        loss = h.sum()
        loss.backward()

        print(f"è¾“å…¥æ¢¯åº¦èŒƒæ•°: {x.grad.norm().item():.6f}")
        # å¾ˆå¤§çš„å€¼ï¼Œè¯´æ˜æ¢¯åº¦çˆ†ç‚¸äº†

demo = GradientFlowDemo()
demo.gradient_vanishing()
demo.gradient_exploding()
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. **æ‰¹å½’ä¸€åŒ–ï¼ˆBatch Normalizationï¼‰**
2. **æ®‹å·®è¿æ¥ï¼ˆResNetï¼‰**
3. **åˆé€‚çš„æƒé‡åˆå§‹åŒ–**
4. **æ¢¯åº¦è£å‰ª**

---

### 3.2 æƒé‡åˆå§‹åŒ–ç­–ç•¥

```python
import math

def xavier_uniform_init(tensor, gain=1.0):
    """
    Xavierå‡åŒ€åˆå§‹åŒ–ï¼ˆé€‚ç”¨äºTanhã€Sigmoidï¼‰
    """
    fan_in, fan_out = tensor.shape
    a = gain * math.sqrt(6.0 / (fan_in + fan_out))
    with torch.no_grad():
        tensor.uniform_(-a, a)

def kaiming_normal_init(tensor, mode='fan_in', nonlinearity='relu'):
    """
    Kaimingæ­£æ€åˆå§‹åŒ–ï¼ˆé€‚ç”¨äºReLUï¼‰
    """
    fan_in, fan_out = tensor.shape
    fan = fan_in if mode == 'fan_in' else fan_out

    # ReLUçš„å¢ç›Šå› å­
    gain = math.sqrt(2.0) if nonlinearity == 'relu' else 1.0
    std = gain / math.sqrt(fan)

    with torch.no_grad():
        tensor.normal_(0, std)

# PyTorchå†…ç½®åˆå§‹åŒ–
layer = nn.Linear(100, 50)
nn.init.xavier_uniform_(layer.weight)
nn.init.zeros_(layer.bias)

# æˆ–è€…
nn.init.kaiming_normal_(layer.weight, mode='fan_in', nonlinearity='relu')
```

<details>
<summary>ğŸ“ æ•°å­¦è¡¥å……ï¼šXavieråˆå§‹åŒ–çš„æ•°å­¦åŸç†ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

**ğŸ’¡ è®°å¿†å£è¯€**ï¼š
```
è¾“å…¥è¾“å‡ºå–å¹³å‡ï¼Œæ–¹å·®å€’æ•°æ˜¯å…³é”®
å‰å‘åå‘éƒ½ç¨³å®šï¼Œä¿¡å·ä¼ æ’­ä¸è¡°å‡
```

**ç›®æ ‡**ï¼šä¿æŒä¿¡å·çš„æ–¹å·®åœ¨å‰å‘å’Œåå‘ä¼ æ’­ä¸­ä¸å˜

**æ¨å¯¼**ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š
```
å‡è®¾è¾“å…¥ x çš„æ–¹å·®ä¸º Var(x) = 1
å±‚è¾“å‡º y = Wxï¼ˆå¿½ç•¥åç½®ï¼‰

Var(y) = Var(Î£áµ¢ wáµ¢xáµ¢)
       = Î£áµ¢ Var(wáµ¢xáµ¢)  ï¼ˆå‡è®¾ç‹¬ç«‹ï¼‰
       = Î£áµ¢ E[wáµ¢Â²]E[xáµ¢Â²]  ï¼ˆå‡è®¾é›¶å‡å€¼ï¼‰
       = n_in Ã— Var(w) Ã— Var(x)

è¦ä½¿ Var(y) = Var(x)ï¼Œéœ€è¦ï¼š
n_in Ã— Var(w) = 1
Var(w) = 1 / n_in

åŒç†ï¼Œè€ƒè™‘åå‘ä¼ æ’­ï¼Œå¾—åˆ°ï¼š
Var(w) = 1 / n_out

æŠ˜ä¸­ï¼šVar(w) = 2 / (n_in + n_out)

å¯¹äºå‡åŒ€åˆ†å¸ƒ U(-a, a)ï¼š
Var = aÂ² / 3
æ‰€ä»¥ a = sqrt(6 / (n_in + n_out))
```

**ç›®çš„**ï¼š
- å¤ªå°ï¼šä¿¡å·é€å±‚è¡°å‡ï¼Œæ¢¯åº¦æ¶ˆå¤±
- å¤ªå¤§ï¼šä¿¡å·é€å±‚æ”¾å¤§ï¼Œæ¢¯åº¦çˆ†ç‚¸
- åˆšå¥½ï¼šä¿¡å·ç¨³å®šä¼ æ’­ï¼Œè®­ç»ƒé¡ºç•…

**é€‚ç”¨åœºæ™¯**ï¼š
- Xavierï¼šé€‚åˆTanhã€Sigmoidç­‰å¯¹ç§°æ¿€æ´»å‡½æ•°
- Kaimingï¼šé€‚åˆReLUï¼ˆä¼šç æ‰ä¸€åŠï¼Œéœ€è¦æ›´å¤§çš„åˆå§‹åŒ–ï¼‰

</details>

---

## ç¬¬å››éƒ¨åˆ†ï¼šå®Œæ•´å®æˆ˜æ¡ˆä¾‹

### 4.1 å›¾åƒåˆ†ç±»ï¼šMNISTæ‰‹å†™æ•°å­—è¯†åˆ«

```python
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms

# 1. æ•°æ®å‡†å¤‡
transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize((0.1307,), (0.3081,))
])

train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)
test_dataset = datasets.MNIST('./data', train=False, transform=transform)

train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
test_loader = DataLoader(test_dataset, batch_size=1000)

# 2. å®šä¹‰æ¨¡å‹
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 32, 3, 1)
        self.conv2 = nn.Conv2d(32, 64, 3, 1)
        self.fc1 = nn.Linear(9216, 128)
        self.fc2 = nn.Linear(128, 10)

    def forward(self, x):
        x = F.relu(self.conv1(x))
        x = F.max_pool2d(x, 2)
        x = F.relu(self.conv2(x))
        x = F.max_pool2d(x, 2)
        x = x.view(-1, 9216)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        return F.log_softmax(x, dim=1)

model = CNN()
optimizer = optim.Adam(model.parameters(), lr=0.001)
criterion = nn.NLLLoss()

# 3. è®­ç»ƒå‡½æ•°
def train(model, device, train_loader, optimizer, epoch):
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        data, target = data.to(device), target.to(device)

        optimizer.zero_grad()
        output = model(data)
        loss = criterion(output, target)
        loss.backward()
        optimizer.step()

        if batch_idx % 100 == 0:
            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)}]'
                  f' Loss: {loss.item():.6f}')

# 4. æµ‹è¯•å‡½æ•°
def test(model, device, test_loader):
    model.eval()
    test_loss = 0
    correct = 0

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()

    test_loss /= len(test_loader)
    accuracy = 100. * correct / len(test_loader.dataset)
    print(f'\nTest set: Average loss: {test_loss:.4f}, '
          f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\n')

# 5. è®­ç»ƒå¾ªç¯
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = model.to(device)

for epoch in range(1, 6):
    train(model, device, train_loader, optimizer, epoch)
    test(model, device, test_loader)

# 6. ä¿å­˜æ¨¡å‹
torch.save(model.state_dict(), 'mnist_cnn.pth')
```

---

## æ€»ç»“ä¸å­¦ä¹ è·¯çº¿å›¾

### ä½ å·²ç»å­¦ä¼šäº†ä»€ä¹ˆ

âœ… **æ ¸å¿ƒæ¦‚å¿µ**ï¼š
- å¼ é‡çš„åˆ›å»ºã€æ“ä½œå’Œå†…å­˜ç®¡ç†
- è‡ªåŠ¨å¾®åˆ†ä¸æ¢¯åº¦è®¡ç®—
- ç¥ç»ç½‘ç»œçš„æ„å»ºä¸è®­ç»ƒ
- GPUåŠ é€Ÿ

âœ… **å®è·µæŠ€èƒ½**ï¼š
- æ•°å€¼ç¨³å®šæ€§çš„å¤„ç†
- æƒé‡åˆå§‹åŒ–ç­–ç•¥
- å®Œæ•´çš„è®­ç»ƒæµç¨‹
- æ¨¡å‹çš„ä¿å­˜ä¸åŠ è½½

### ä¸‹ä¸€æ­¥å­¦ä¹ æ–¹å‘

ğŸ“– **æ•™ç¨‹02 - å¼ é‡è¿ç®—ä¸è‡ªåŠ¨å¾®åˆ†æ·±åº¦è§£æ**ï¼š
- é«˜çº§ç´¢å¼•ä¸å¹¿æ’­æœºåˆ¶
- è‡ªå®šä¹‰autogradå‡½æ•°
- å†…å­˜ä¼˜åŒ–æŠ€å·§

ğŸ“– **æ•™ç¨‹03 - ç¥ç»ç½‘ç»œæ¶æ„è®¾è®¡**ï¼š
- CNNã€RNNã€Transformerè¯¦è§£
- è‡ªå®šä¹‰ç½‘ç»œå±‚
- æ¨¡å‹ç»„åˆæŠ€å·§

ğŸ“– **æ•™ç¨‹04 - ä¼˜åŒ–å™¨ä¸è®­ç»ƒæŠ€å·§**ï¼š
- å„ç§ä¼˜åŒ–å™¨çš„æ•°å­¦åŸç†
- å­¦ä¹ ç‡è°ƒåº¦
- æ­£åˆ™åŒ–æŠ€æœ¯

ğŸ“– **æ•™ç¨‹05 - åˆ†å¸ƒå¼è®­ç»ƒä¸æ€§èƒ½ä¼˜åŒ–**ï¼š
- æ•°æ®å¹¶è¡Œä¸æ¨¡å‹å¹¶è¡Œ
- æ··åˆç²¾åº¦è®­ç»ƒ
- æ€§èƒ½åˆ†æå·¥å…·

ğŸ“– **æ•™ç¨‹06 - å®æˆ˜åº”ç”¨æ¡ˆä¾‹**ï¼š
- è®¡ç®—æœºè§†è§‰
- è‡ªç„¶è¯­è¨€å¤„ç†
- ç”Ÿæˆæ¨¡å‹

---

## é™„å½•ï¼šå¸¸ç”¨æ•°å­¦çŸ¥è¯†é€ŸæŸ¥

<details>
<summary>ğŸ“ çº¿æ€§ä»£æ•°åŸºç¡€ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

### å‘é‡è¿ç®—
```python
# å‘é‡ç‚¹ç§¯
a = torch.tensor([1., 2., 3.])
b = torch.tensor([4., 5., 6.])
dot_product = (a * b).sum()  # 32.0

# å‘é‡èŒƒæ•°
l1_norm = a.abs().sum()       # L1èŒƒæ•°: 6.0
l2_norm = a.norm()            # L2èŒƒæ•°: 3.742
```

### çŸ©é˜µè¿ç®—
```python
# çŸ©é˜µä¹˜æ³•
A = torch.randn(3, 4)
B = torch.randn(4, 5)
C = A @ B  # (3, 5)

# çŸ©é˜µè½¬ç½®
A_T = A.T

# é€†çŸ©é˜µ
A_square = torch.randn(3, 3)
A_inv = torch.inverse(A_square)
```

### ç‰¹å¾å€¼åˆ†è§£
```python
A = torch.randn(3, 3)
eigenvalues, eigenvectors = torch.linalg.eig(A)
```

</details>

<details>
<summary>ğŸ“ å¾®ç§¯åˆ†åŸºç¡€ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

### å¸¸ç”¨å¯¼æ•°
```
d/dx (x^n) = nx^(n-1)
d/dx (e^x) = e^x
d/dx (ln x) = 1/x
d/dx (sin x) = cos x
d/dx (cos x) = -sin x
```

### é“¾å¼æ³•åˆ™
```
d/dx f(g(x)) = f'(g(x)) Ã— g'(x)
```

### å¸¸ç”¨æ¿€æ´»å‡½æ•°çš„å¯¼æ•°
```python
# Sigmoid: Ïƒ(x) = 1/(1+e^(-x))
# å¯¼æ•°: Ïƒ'(x) = Ïƒ(x)(1-Ïƒ(x))

# ReLU: f(x) = max(0, x)
# å¯¼æ•°: f'(x) = 1 if x>0 else 0

# Tanh: tanh(x)
# å¯¼æ•°: 1 - tanhÂ²(x)
```

</details>

<details>
<summary>ğŸ“ æ¦‚ç‡è®ºåŸºç¡€ï¼ˆç‚¹å‡»å±•å¼€ï¼‰</summary>

### å¸¸ç”¨åˆ†å¸ƒ
```python
# æ­£æ€åˆ†å¸ƒ
x = torch.randn(1000)  # N(0, 1)

# å‡åŒ€åˆ†å¸ƒ
x = torch.rand(1000)   # U(0, 1)

# ä¼¯åŠªåˆ©åˆ†å¸ƒ
x = torch.bernoulli(torch.full((1000,), 0.5))
```

### æœŸæœ›ä¸æ–¹å·®
```python
# æœŸæœ›ï¼ˆå‡å€¼ï¼‰
mean = x.mean()

# æ–¹å·®
var = x.var()

# æ ‡å‡†å·®
std = x.std()
```

</details>

---

**æ­å–œä½ å®Œæˆç¬¬ä¸€ç« çš„å­¦ä¹ ï¼** ğŸ‰

ç°åœ¨ä½ å·²ç»æŒæ¡äº†PyTorchçš„æ ¸å¿ƒåŸºç¡€ï¼Œå¯ä»¥å¼€å§‹æ„å»ºè‡ªå·±çš„æ·±åº¦å­¦ä¹ æ¨¡å‹äº†ã€‚è®°ä½ï¼š
- å¤šåŠ¨æ‰‹å®è·µï¼Œç†è®ºç»“åˆä»£ç 
- é‡åˆ°æ•°å­¦æ¦‚å¿µæ—¶ï¼ŒæŸ¥çœ‹è¡¥å……è¯´æ˜
- å¾ªåºæ¸è¿›ï¼Œä¸è¦æ€¥äºæ±‚æˆ

ç»§ç»­åŠ æ²¹ï¼ğŸ’ª
